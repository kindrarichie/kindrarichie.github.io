---
layout: default
---

## Blog Post 11

Module 9 was the focus of this week. I also added system monitoring to the [wiki](https://github.com/alexcoward/Project2/wiki/System-Monitoring) for the group project. 

---

### AWS Module 9: Cloud Architecture


<br>


#### Topics

-**AWS Well-Architected Framework** 
<br>
Architecture is the designing and building large structures. Large systems require architects to manage their size and complexity.
<br>
Cloud architects: 
<br>
• Engage with decision makers to identify the business goal and the capabilities that need improvement.
<br>
• Ensure alignment between technology deliverables of a solution and the business goals. 
<br>
• Work with delivery teams that are implementing the solution to ensure that the technology features are appropriate.

The AWS Well-Architected Framework is a guide that is designed to help you build the most secure, high-performing, resilient, and efficient infrastructure possible for your cloud applications and workloads. It provides a set of foundational questions and best practices. 

The AWS Well-Architected Framework is organized into five pillars: operational excellence, security, reliability, performance efficiency, and cost optimization.

Each pillar includes a set of design principles and best practice areas. A set of foundational questions is under each best practice area. 

The Operational Excellence pillar focuses on the ability to run and monitor systems to deliver business value, and to continually improve supporting processes and procedures. Key topics include: managing and automating changes, responding to events, and defining standards to successfully manage daily operations.


There are six design principles for operational excellence in the cloud: 
<br>
• Perform operations as code – Define your entire workload (that is, applications and infrastructure) as code and update it with code. Implement operations procedures as code and configure them to automatically trigger in response to events. You limit human error and enable consistent responses to events.
<br>
• Annotate documentation – Automate the creation of annotated documentation after every build. Annotated documentation can be used by people and systems. They can be used as input to your operations code.
<br>
• Make frequent, small, reversible changes – Design workloads to enable components to be updated regularly. Make changes in small increments that can be reversed if they fail.
<br>
• Refine operations procedures frequently – Look for opportunities to improve operations procedures. Evolve your procedures appropriately as your workloads evolve. Set up regular game days to review all procedures, validate their effectiveness, and ensure that teams are familiar with them.
<br>
• Anticipate failure – Identify potential sources of failure so that they can be removed or mitigated. Test failure scenarios and validate your understanding of their impact. Test your response procedures to ensure that they are effective and that teams are familiar with their execution. Set up regular game days to test workloads and team responses to simulated events.
<br>
• Learn from all operational failures – Drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization.

The foundational questions for operational excellence fall under three best practice areas: prepare, operate, and evolve.

Operations teams must understand business and customer needs so they can effectively and efficiently support business outcomes. Operations teams create and use procedures to respond to operational events and validate the effectiveness of procedures to support business needs. Operations teams collect metrics that are used to measure the achievement of desired business outcomes. 
<br>

The Security pillar focuses on the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. Key topics include: protecting confidentiality and integrity of data, identifying and managing who can do what (or privilege management), protecting systems, and establishing controls to detect security events.

There are seven design principles that can improve security: 
<br>
• Implement a strong identity foundation – Implement the principle of least privilege and enforce separation of duties with appropriate authorization for each interaction with your AWS resources. Centralize privilege management and reduce or even eliminate reliance on long-term credentials.
<br>
• Enable traceability – Monitor, alert, and audit actions and changes to your environment in real time. Integrate logs and metrics with systems to automatically respond and take action.
<br>
• Apply security at all layers – Apply defense in depth and apply security controls to all layers of your architecture (for example, edge network, virtual private cloud, subnet, and load balancer; and every instance, operating system, and application).
<br>
• Automate security best practices – Automate security mechanisms to improve your ability to securely scale more rapidly and cost effectively. Create secure architectures and implement controls that are defined and managed as code in version-controlled templates.
<br>
• Protect data in transit and at rest – Classify your data into sensitivity levels and use mechanisms such as encryption, tokenization, and access control where appropriate.
<br>
• Keep people away from data – To reduce the risk of loss or modification of sensitive data due to human error, create mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data.
<br>
• Prepare for security events – Have an incident management process that aligns with organizational requirements. Run incident response simulations and use tools with automation to increase your speed of detection, investigation, and recovery.

The foundational questions for security fall under five best practice areas: identity and access management, detective controls, infrastructure protection, data protection, and incident response.
Before you architect any system, you must put security practices in place. You must be able to control who can do what. In addition, you must be able to identify security incidents, protect your systems and services, and maintain the confidentiality and integrity of data through data protection. You should have a well-defined and practiced process for responding to security incidents. 
<br>

The Reliability pillar focuses on the ability of a system to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions such as misconfigurations or transient network issues. Key topics include: set up, cross-project requirements, recovery planning, and handling change.

There are five design principles that can increase reliability: 
<br>
• Test recovery procedures – Test how your systems fail and validate your recovery
procedures. Use automation to simulate different failures or to recreate scenarios that led to failures before. This practice can expose failure pathways that you can test and rectify before a real failure scenario.
<br>
• Automatically recover from failure – Monitor systems for key performance indicators and configure your systems to trigger an automated recovery when a threshold is breached. This practice enables automatic notification and failure-tracking, and for automated recovery processes that work around or repair the failure.
<br>
• Scale horizontally to increase aggregate system availability – Replace one large resource with multiple, smaller resources and distribute requests across these smaller resources to reduce the impact of a single point of failure on the overall system.
<br>
• Stop guessing capacity – Monitor demand and system usage, and automate the addition or removal of resources to maintain the optimal level for satisfying demand.
<br>
• Manage change in automation – Use automation to make changes to infrastructure and manage changes in automation.

The foundational questions for reliability fall under three best practice areas: foundations, change management, and failure management. To achieve reliability, a system must have both a well-planned foundation and monitoring in place. It must have mechanisms for handling changes in demand or requirements. The system should be designed to detect failure and automatically heal itself.
<br>

The Performance Efficiency pillar focuses on the ability to use IT and computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes or technologies evolve. Key topics include: selecting the right resource types and sizes based on workload requirements, monitoring performance, and making informed decisions to maintain efficiency as business needs evolve.


There are five design principles that can improve performance efficiency: 
<br>
• Democratize advanced technologies – Consume technologies as a service. For example, technologies such as NoSQL databases, media transcoding, and machine learning require expertise that is not evenly dispersed across the technical community. In the cloud, these technologies become services that teams can consume. Consuming technologies enables teams to focus on product development instead of resource provisioning and management.
<br>
• Go global in minutes – Deploy systems in multiple AWS Regions to provide lower latency and a better customer experience at minimal cost.
<br>
• Use serverless architectures – Serverless architectures remove the operational burden of running and maintaining servers to carry out traditional compute activities. Serverless architectures can also lower transactional costs because managed services operate at cloud scale.
<br>
• Experiment more often – Perform comparative testing of different types of instances, storage, or configurations.
<br>
• Have mechanical sympathy – Use the technology approach that aligns best to what you are trying to achieve. For example, consider your data access patterns when you select approaches for databases or storage.

The foundational questions for performance efficiency fall under four best practice areas: selection, review, monitoring, and tradeoffs. Use data to design and build a high-performance architecture. Gather data on all aspects of the architecture, from the high-level design to the selection and configuration of resource types. Review your choices periodically to ensure that you are taking advantage of new AWS services. Perform monitoring so that you are aware of any deviance from expected performance and can take prompt action to remediate them. Finally, use tradeoffs in your architecture to improve performance, such as using compression, using caching, or relaxing consistency requirements.
<br>


The Cost Optimization pillar focuses on the ability to run systems to deliver business value at the lowest price point. Key topics include: understanding and controlling when money is being spent, selecting the most appropriate and right number of resource types, analyzing spending over time, and scaling to meeting business needs without overspending.

There are five design principles that can optimize costs: 
<br>
• Adopt a consumption model – Pay only for the computing resources that you require. Increase or decrease usage depending on business requirements, not by using elaborate forecasting.
<br>
• Measure overall efficiency – Measure the business output of the workload and the costs that are associated with delivering it. Use this measure to know the gains that you make from increasing output and reducing costs.
<br>
• Stop spending money on data center operations – AWS does the heavy lifting of racking, stacking, and powering servers, which means that you can focus on your customers and business projects instead of the IT infrastructure.
<br>
• Analyze and attribute expenditure – The cloud makes it easier to accurately identify system usage and costs, and attribute IT costs to individual workload owners. Having this capability helps you measure return on investment (ROI) and gives workload owners an opportunity to optimize their resources and reduce costs.
<br>
• Use managed and application-level services to reduce cost of ownership – Managed and application-level services reduce the operational burden of maintaining servers for tasks such as sending email or managing databases. Because managed services operate at cloud scale, cloud service providers can offer a lower cost per transaction or service.

The foundational questions for cost optimization fall under four best practice areas: expenditure awareness, cost-effective resources, matching supply and demand, and optimizing over time.
Similar to the other pillars, there are tradeoffs to consider when evaluating cost. You should identify your true application needs and use empirical data to inform your architectural design decisions. Perform benchmarking to establish the most cost-optimal workload over time.

The AWS Well-Architected Tool helps you review the state of your workloads and compare them to the latest AWS architectural best practices. It gives you access to knowledge and best practices used by AWS architects, whenever you need it. This tool is available in the AWS Management Console. 

Key takeaways: 
<br>
• The AWS Well-Architected Framework documents a set of foundational questions that enable you to understand if a specific architecture aligns well with cloud best practices.
<br>
• The AWS Well-Architected Framework is organized into five pillars: operational excellence, security, reliability, performance efficiency, and cost optimization. 
<br>
• Each pillar includes a set of design principles and best practices.
<br>
<br>
-**Reliability and availability** 
<br>
One of the best practices that is identified in the AWS Well-Architected Framework is to plan for failure (or application or workload downtime). One way to do that is to architect your applications and workloads to withstand failure. There are two important factors that cloud architects consider when designing architectures to withstand failure: reliability and availability.

Reliability is a measure of your system’s ability to provide functionality when desired by the user.  Reliability is the probability that an entire system will function as intended for a specified period.  Failure of system components impacts the availability of the system. A common way to measure reliability is to use statistical measurements, such as Mean Time Between Failures (MTBF). MTBF is the total time in service over the number of failures. It is the sum of MTTF and MTTR.

Formally, availability is the percentage of time that a system is operating normally or correctly performing the operations expected of it (or normal operation time over total time). Availability is reduced anytime the application isn’t operating normally, including both scheduled and unscheduled interruptions. Availability is also defined as the percentage of uptime (that is, length of time that a system is online between failures) over a period of time (commonly 1 year).
A common shorthand when referring to availability is number of 9s. For example, five 9s means 99.999 percent availability.

A highly available system is one that can withstand some measure of degradation while still remaining available. In a highly available system, downtime is minimized as much as possible and minimal human intervention is required. A highly available system can be viewed as a set of system-wide, shared resources that cooperate to guarantee essential services. High availability combines software with open-standard hardware to minimize downtime by quickly restoring essential services when a system, component, or application fails. Services are restored rapidly.

Availability requirements vary. The length of disruption that is acceptable depends on the type of application. 

Though events that might disrupt an application’s availability cannot always be predicted, you can build availability into your architecture design. There are three factors that determine the overall availability of your application:
<br>
• Fault tolerance refers to the built-in redundancy of an application's components and the ability of the application to remain operational even if some of its components fail. The fault-tolerant model does not address software failures, which are the most common reason for downtime.
<br>
• Scalability is the ability of your application to accommodate increases in capacity needs, remain available, and perform within your required standards.
<br>
• Recoverability is the ability to restore service quickly and without lost data if a disaster makes your components unavailable, or it destroys data.

Keep in mind that improving availability usually leads to increased cost.

Key takeaways: 
<br>
• Reliability is a measure of your system’s ability to provide functionality when desired by the user, and it can be measured in terms of MTBF.
<br>
• Availability is the percentage of time that a system is operating normally or correctly performing the operations expected of it (or normal operation time over total time).
<br>
• Three factors that influence the availability of your applications are fault tolerance, scalability, and recoverability.
<br>
• You can design your workloads and applications to be highly available, but there is a cost tradeoff to consider.
<br>
<br>

-**AWS Trusted Advisor** 
<br>
AWS Trusted Advisor is an online tool that provides real-time guidance to help you provision your resources following AWS best practices. AWS Trusted Advisor looks at your entire AWS environment and gives you recommendations in five categories: 
<br>
• Cost Optimization – AWS Trusted Advisor looks at your resource use and makes recommendations to help you optimize cost by eliminating unused and idle resources, or by making commitments to reserved capacity.
<br>
• Performance – Improve the performance of your service by checking your service limits, ensuring you take advantage of provisioned throughput, and monitoring for overutilized instances.
<br>
• Security – Improve the security of your application by closing gaps, enabling various AWS security features, and examining your permissions.
<br>
• Fault Tolerance – Increase the availability and redundancy of your AWS application by taking advantage of automatic scaling, health checks, Multi-AZ deployments, and backup capabilities.
<br>
• Service Limits – AWS Trusted Advisor checks for service usage that is more than 80 percent of the service limit. Values are based on a snapshot, so your current usage might differ. Limit and usage data can take up to 24 hours to reflect any changes.

Key takeaways: 
<br>
• AWS Trusted Advisor is an online tool that provides real-time guidance to help you provision your resources by following AWS best practices.
<br>
• AWS Trusted Advisor looks at your entire AWS environment and gives you real-time recommendations in five categories.
<br>
• You can use AWS Trusted Advisor to help you optimize your AWS environment as soon as you start implementing your architecture designs.
<br>
<br>





[back](../blog.html)
