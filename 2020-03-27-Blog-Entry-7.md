---
layout: default
---

## Blog Post 7

This week we resumed coursework after Spring Break. Classes are now online due to social distancing to control the spread of COVID-19. 

I continued the AWS training and completed some of the [documentation](https://github.com/alexcoward/Project2/wiki) necessary for the group project. 

---

### AWS Module 5: Networking and Content Delivery

#### Topics

-**Networking Basics** A computer network is two or more client machines that are connected
together to share resources. A network can be logically partitioned into subnets. Networking
requires a networking device (ex: router) to connect all the clients together and enable
communication between them. Each client machine in a network has a unique Internet Protocol
(IP) address that identifies it. A 32-bit IP address is called an IPv4 address. IPv6 addresses, which
are 128 bits, are also available. IPv6 addresses can accommodate more user devices. A common
method to describe networks is Classless Inter-Domain Routing (CIDR). CIDR is a way to express
a group of IP addresses that are consecutive to each other. There is a number that tells you how many bits of the routing prefix must be fixed or allocated for the network identifier. The bits that are not fixed are allowed to change. There are two special cases: 1 Fixed IP addresses, in which every bit is fixed, represent a single IP address (for example, 192.0.2.0/32). This type of address is helpful when you want to set up a firewall rule and give access to a specific host. 2 The internet, in which every bit is flexible, is represented as 0.0.0.0/0. The Open Systems Interconnection (OSI) model is a conceptual model that is used to explain how data travels over a network. It consists of seven layers and shows the common protocols and addresses that are used to send data at each layer. 
<br>
<br>
-**Amazon VPC** 
Amazon Virtual Private Cloud (Amazon VPC) is a service that lets you provision a logically isolated section of the AWS Cloud (called a virtual private cloud, or VPC) where you can launch your AWS resources. Amazon VPC gives you control over your virtual networking resources. You can customize the network configuration for your VPC and you can use multiple layers of security, including security groups and network access control lists. A VPC is a virtual network that is logically isolated from other virtual networks in the AWS Cloud. A VPC is dedicated to your account. VPCs belong to a single AWS Region and can span multiple Availability Zones.
After you create a VPC, you can divide it into one or more subnets. A subnet is a range of IP addresses in a VPC. Subnets belong to a single Availability Zone. You can create subnets in different Availability Zones for high availability. Subnets are generally classified as public or private. Public subnets have direct access to the internet, but private subnets do not. When you create a VPC, you assign an IPv4 CIDR block (a range of private IPv4 addresses) to it. After you create a VPC, you cannot change the address range, so it is important that you choose it carefully. If you create more than one subnet in a VPC, the CIDR blocks of the subnets cannot overlap. You cannot have duplicate IP addresses in the same VPC. For each CIDR block that you specify, AWS reserves five IP addresses within that block, and these addresses are not available for use. 

An Elastic IP address is a static and public IPv4 address that is designed for dynamic cloud computing. You can associate an Elastic IP address with any instance or network interface for any VPC in your account. With an Elastic IP address, you can mask the failure of an instance by rapidly remapping the address to another instance in your VPC. Associating the Elastic IP address with the network interface has an advantage over associating it directly with the instance. You can move all of the attributes of the network interface from one instance to another in a single step.

An elastic network interface is a virtual network interface that you can attach or detach from an instance in a VPC. A network interface's attributes follow it when it is reattached to another instance. When you move a network interface from one instance to another, network traffic is redirected to the new instance.

A route table contains a set of rules (called routes) that directs network traffic from your subnet. Each route specifies a destination and a target. The destination is the destination CIDR block where you want traffic from your subnet to go. The target is the target that the destination traffic is sent through. 

Each subnet in your VPC must be associated with a route table. The main route table is the route table is automatically assigned to your VPC. It controls the routing for all subnets that are not explicitly associated with any other route table. A subnet can be associated with only one route table at a time.
<br>
<br>
-**VPC Networking**
An internet gateway is a scalable, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation for instances that were assigned public IPv4 addresses.
To make a subnet public, you attach an internet gateway to your VPC and add a route to the route table to send non-local traffic through the internet gateway to the internet.

A network address translation (NAT) gateway enables instances in a private subnet to connect to the internet or other AWS services, but prevents the internet from initiating a connection with those instances. To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. After you create a NAT gateway, you must update the route table that is associated with one or more of your private subnets to point internet-bound traffic to the NAT gateway. You can also use a NAT instance in a public subnet in your VPC instead of a NAT gateway. However, a NAT gateway is a managed NAT service that provides better availability, higher bandwidth, and less administrative effort. 

VPC sharing enables customers to share subnets with other AWS accounts in the same organization in AWS Organizations. VPC sharing enables multiple AWS accounts to create their application resources—such as Amazon EC2 instances, Amazon Relational Database Service (Amazon RDS) databases, Amazon Redshift clusters, and AWS Lambda functions—into shared, centrally managed VPCs. VPC sharing offers several benefits: 
<br>
<br>
-Separation of duties, centrally controlled VPC structure, routing, IP address allocation
<br>
Ownership, application owners continue to own resources, accounts, and security groups
<br>
-Security groups, VPC sharing participants can reference the security group IDs of each other
<br>
-Efficiencies, higher density in subnets, efficient use of VPNs and AWS Direct Connect
<br>
-No hard limits, hard limits can be avoided
<br>
-Optimized costs, costs can be optimized through the reuse of NAT gateways, VPC interface endpoints, and intra-Availability Zone traffic

A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.

By default, instances that you launch into a VPC cannot communicate with a remote network. To connect your VPC to your remote network (that is, create a virtual private network or VPN connection), you: 
<br>
<br>
-Create a new virtual gateway device (called a virtual private network (VPN) gateway) and attach it to your VPC.
<br>
-Define the configuration of the VPN device or the customer gateway. 
<br>
Create a custom route table to point corporate data center-bound traffic to the VPN gateway. You also must update security group rules. 
<br>
-Establish an AWS Site-to-Site VPN (Site-to-Site VPN) connection to link the two systems together.
<br>

One of the challenges of network communication is network performance. Performance can be negatively affected if your data center is located far away from your AWS Region. For such situations, AWS offers AWS Direct Connect, or DX. AWS Direct Connect enables you to establish a dedicated, private network connection between your network and one of the DX locations. 

A VPC endpoint is a virtual device that enables you to privately connect your VPC to supported AWS services and VPC endpoint services that are powered by AWS PrivateLink. Traffic between your VPC and the other service does not leave the Amazon network.
There are two types of VPC endpoints: 1 An interface VPC endpoint (interface endpoint), enables you to connect to services that are powered by AWS PrivateLink. You are charged for creating and using an interface endpoint to a service. Hourly usage rates and data processing rates apply. 2 Gateway endpoints, the use of gateway endpoints incurs no additional charge. 

You can configure your VPCs in several ways, and take advantage of numerous connectivity options and gateways. Things get more complex when customers start to set up connectivity between their VPCs. To solve this problem, you can use AWS Transit Gateway to simplify your networking model. With AWS Transit Gateway, you only need to create and manage a single connection from the central gateway into each VPC, on-premises data center, or remote office across your network.  This ease of connectivity makes it easier to scale your network as you grow.
<br>
<br>
-**VPC Security** A security group acts as a virtual firewall for your instance, and it controls inbound and outbound traffic. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups. A security group is a way for you to filter traffic to your instances. Security groups have rules that control the inbound and outbound traffic. Security groups are stateful, which means that state information is kept even after a request is processed. When you create a custom security group, you can specify allow rules, but not deny rules. All rules are evaluated before the decision to allow traffic.

A network access control list (network ACL) is an optional layer of security for your Amazon VPC. It acts as a firewall for controlling traffic in and out of one or more subnets. To add another layer of security to your VPC, you can set up network ACLs with rules that are similar to your security groups. Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL. You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed. A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. Network ACLs are stateless, which means that no information about a request is maintained after a request is processed. You can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until you add rules. A network ACL contains a numbered list of rules that are evaluated in order, starting with the lowest numbered rule. The highest number that you can use for a rule is 32,766. AWS recommends that you create rules in increments so that you can insert new rules where you need them later.

Summary of the differences between security groups and network ACLs:
<br>
<br>
-Security groups act at the instance level, but network ACLs act at the subnet level.
<br>
-Security groups support allow rules only, but network ACLs support both allow and deny rules.
<br>
-Security groups are stateful, but network ACLs are stateless.
<br>
-For security groups, all rules are evaluated before the decision is made to allow traffic. For network ACLs, rules are evaluated in number order before the decision is made to allow traffic.
<br>
<br>
-**Route 53** Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS. You can use Amazon Route 53 to configure DNS health checks so you that can route traffic to healthy endpoints or independently monitor the health of your application and its endpoints. Amazon Route 53 traffic flow helps you manage traffic globally through several routing types, which can be combined with DNS failover to enable various low-latency, fault-tolerant architectures. Amazon Route 53 also offers Domain Name Registration. When a user initiates a DNS request, the DNS resolver checks with your domain in Route 53, gets the IP address, and returns it to the user. Amazon Route 53 supports several types of routing policies, which determine how Amazon Route 53 responds to queries:
<br>
<br>
-Simple routing (round robin), use for a single resource that performs a given function for your domain (such as a web server that serves content for the example.com website).
<br>
-Weighted round robin routing, use to route traffic to multiple resources in proportions that you specify. Enables you to assign weights to resource record sets to specify the frequency with which different responses are served. You might want to use this capability to do A/B testing, which is when you send a small portion of traffic to a server where you made a software change. <br>
-Latency routing (LBR), use when you have resources in multiple AWS Regions and you want to route traffic to the Region that provides the best latency. Latency routing works by routing your customers to the AWS endpoint that provides the fastest experience based on actual performance measurements of the different AWS Regions where your application runs.
<br>
-Geolocation routing, use when you want to route traffic based on the location of your users. When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users.
<br>
-Geoproximity routing, use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.
-Failover routing (DNS failover), use when you want to configure active-passive failover. Amazon Route 53 can help detect an outage of your website and redirect your users to alternate locations where your application is operating properly. 
<br>
-Multivalue answer routing, use when you want Route 53 to respond to DNS queries with up to eight healthy records that are selected at random. You can specify multiple values for almost any record, but multivalue answer routing also enables you to check the health of each resource so that Route 53 returns only values for healthy resources.

Multi-Region deployment is an example use case for Amazon Route 53. With Amazon Route 53, the user is automatically directed to the Elastic Load Balancing load balancer that’s closest to the user.

Amazon Route 53 enables you to improve the availability of your applications that run on AWS by:
<br>
<br>
-Configuring backup and failover scenarios for your own applications.
<br>
-Enabling highly available multi-Region architectures on AWS.
<br>
-Creating health checks to monitor the health and performance of your web applications, web servers, and other resources.
<br>
<br>
-**CloudFront** The number of network hops and the distance that requests must travel significantly affect the performance and responsiveness of websites. Further, network latency is different in various geographic locations. For these reasons, a content delivery network might be the solution.

A content delivery network (CDN) is a globally distributed system of caching servers. A CDN caches copies of commonly requested files that are hosted on the application origin server. The CDN delivers a local copy of the requested content from a cache edge or Point of Presence that provides the fastest delivery to the requester. CDNs also deliver dynamic content that is unique to the requester and is not cacheable. Having a CDN deliver dynamic content improves application performance and scaling. Also, content such as form data, images, and text can be ingested and sent back to the origin, thus taking advantage of the low-latency connections and proxy behavior of the PoP.

Amazon CloudFront is a fast CDN service that securely delivers data, videos, applications, and application programming interfaces (APIs) to customers globally with low latency and high transfer speeds. Amazon CloudFront delivers files to users over a global network of edge locations and Regional edge caches (for the less popular content). 

Amazon CloudFront provides the following benefits:
<br>
<br>
-Fast and global 
<br>
-Security at the edge
<br>
-Highly programmable 
<br>
-Deeply integrated with AWS 
<br>
-Cost-effective 

Amazon CloudFront charges are based on actual usage of the service in four areas: 
<br>
<br>
-Data transfer out 
<br>
-HTTP(S) requests 
<br>
-Invalidation requests 
<br>
-Dedicated IP custom Secure Sockets Layer (SSL)
<br>
<br>


---

### Lab 6: Scale and Load Balance Your Architecture

Lab 6 illustrates the use of Elastic Load Balancing (ELB) and Auto Scaling to load balance as well as automatically scale infrastructure. Elastic Load Balancing automatically distributes application traffic across Amazon EC2 instances. Auto Scaling helps to maintain application availability and allows you to scale your Amazon EC2 capacity automatically according to conditions you define.

#### Topics

-Create an Amazon Machine Image (AMI) from a running instance
<br>
-Create a load balancer
<br>
-Create a launch configuration and an Auto Scaling group
<br>
-Automatically scale new instances within a private subnet 
<br>
-Create Amazon CloudWatch alarms and monitor performance of your infrastructure

#### Tasks

**Create an AMI for Auto Scaling**
<br>
-In the AWS Management Console, on the Services menu, click EC2.
<br>
-In the left navigation pane, click Instances.
<br>
-In the Actions menu, click Image > Create Image and configure.
<br>
**Create a Load Balancer**
<br>
-In the left navigation pane, click Load Balancers. Click Create Load Balancer. Select Application Load Balancer and click Create. Then configure.
<br>
**Create a Launch Configuration and an Auto Scaling Group**
<br>
-In the left navigation pane, click Launch Configurations. Click Create launch configuration.
<br>
-In the left navigation pane, click My AMIs. In the row for Web Server AMI, click Select.
<br>
-Select the t2.micro instance type and click Next: Configure details. Configure settings. 
<br>
-Click Next: Add Storage. Click Next: Configure Security Group. Configure settings.
<br>
-Review the details of your launch configuration and click Create launch configuration.
<br>
-Click Create an Auto Scaling group using this launch configuration. Configure settings.
<br>
-Expand Advanced Details, then configure. Select Receive traffic from one or more load balancers. Select Enable CloudWatch detailed monitoring.
<br>
-Click Next: Configure scaling policies. Select Use scaling policies to adjust the capacity of this group. Modify the Scale. In Scale Group Size, Metric type: Average CPU Utilization. Target value: 60. This tells Auto Scaling to maintain an average CPU utilization across all instances at 60%. 
<br>
-Click Next: Configure Notifications. 
<br>
-Click Next: Configure Tags. 
<br>
-Click Review then click Create Auto Scaling group.
<br>
**Verify that Load Balancing is Working**
<br>
-In the left navigation pane, click Instances. You should see two new instances named Lab Instance launched by Auto Scaling.
<br>
-In the left navigation pane, click Target Groups. Click the Targets tab. Two Lab Instance targets should be listed for this target group. Wait until the Status of both instances transitions to healthy.
<br>
-In the left navigation pane, click Load Balancers. Copy the DNS name of the load balancer.
<br>
-Open a new web browser tab, paste the DNS Name you just copied, and press Enter. The application should appear in your browser indicating that the Load Balancer received the request, sent it to one of the EC2 instances, then passed back the result.
<br>
**Test Auto Scaling**
<br>
-In the AWS management console, on the Services menu, click CloudWatch.
<br>
-In the left navigation pane, click Alarms.
<br>
-Switch to the application browser tab and Click Load Test beside the AWS logo.
<br>
-Return to browser tab with the CloudWatch console. The AlarmLow alarm should change to OK and the AlarmHigh alarm status should change to ALARM.
<br>
-You can now view the additional instance(s) that were launched. On the Services menu, click EC2. In the left navigation pane, click Instances. More than two instances labeled Lab Instance should now be running.
<br>
**Terminate Web Server 1**
<br>
-Select Web Server 1. 
<br>
-In the Actions menu, click Instance State > Terminate.
<br>
<br>

---

### Activity: AWS Elastic Beanstalk

For this activity we deployed code to an AWS Elastic Beanstalk environment and observed the resources.

#### Tasks

**Access the Elastic Beanstalk environment**
<br>
-In the AWS Management Console, on the Services menu, choose Elastic Beanstalk.
<br>
-In the green application details box, click the URL value that displays.
<br>
-Near the top of the page, click the URL (the URL ends in elasticbeanstalk.com). A new browser tab opens. Return to the Elastic Beanstalk console.
<br>
**Deploy a sample application to Elastic Beanstalk** 
<br>
-Download a sample application. Back in the Elastic Beanstalk console, click Upload and Deploy.
<br>
-After the deployment is complete, click the URL value near the top of the screen to display the deployed web application.
<br>
-Back in the Elastic Beanstalk console, click Configuration.
<br>
-Click Monitoring. Browse through the charts to see the kinds of information that are available to you.
<br>
**Explore the AWS resources that support your application**
<br>
-From the Services menu, choose EC2, click Instances.
<br>
-There are two instances running that support your web application.
<br>




[back](../blog.html)
