---
layout: default
---

## Blog Post 8

Coursework has continued online due to COVID-19. 

I continued the AWS training. 

---

### AWS Module 6: Compute

#### Topics

-**Compute Services Overview** 
Summary of what each compute service offers:
<br>
<br>
• Amazon Elastic Compute Cloud (Amazon EC2) provides resizable virtual machines. 
<br>
• Amazon EC2 Auto Scaling supports application availability by allowing you to define conditions that will automatically launch or terminate EC2 instances.
<br>
• Amazon Elastic Container Registry (Amazon ECR) is used to store and retrieve Docker images.
<br>
• Amazon Elastic Container Service (Amazon ECS) is a container orchestration service that supports Docker.
<br>
• VMware Cloud on AWS enables you to provision a hybrid cloud without custom hardware.
<br>
• AWS Elastic Beanstalk provides a simple way to run and manage web applications. 
<br>
• AWS Lambda is a serverless compute solution. You pay only for the compute time that you use.
<br>
• Amazon Elastic Kubernetes Service (Amazon EKS) enables you to run managed Kubernetes on AWS.
<br>
• Amazon Lightsail provides a simple-to-use service for building an application or website. • AWS Batch provides a tool for running batch jobs at any scale. 
<br>
• AWS Fargate provides a way to run containers that reduce the need for you to manage servers or clusters.
<br>
• AWS Outposts provides a way to run select AWS services in your on-premises data center. 
<br>
• AWS Serverless Application Repository provides a way to discover, deploy, and publish serverless applications.

Each AWS compute service belongs to one of four broad categories: virtual machines (VMs) that provide infrastructure as a service (IaaS), serverless, container-based, and platform as a service (PaaS).

Amazon EC2 provides virtual machines, and you can think of it as infrastructure as a service (IaaS). IaaS services provide flexibility and leave many of the server management responsibilities to you. 

AWS Lambda is a zero-administration compute platform. AWS Lambda enables you to run code without provisioning or managing servers. You pay only for the compute time that is consumed. 

Container-based services—including Amazon Elastic Container Service, Amazon Elastic Kubernetes Service, AWS Fargate, and Amazon Elastic Container Registry—enable you to run multiple workloads on a single operating system (OS). Containers spin up more quickly than virtual machines, thus offering responsiveness.

AWS Elastic Beanstalk provides a platform as a service (PaaS). It facilitates the quick deployment of applications that you create by providing all the application services that you need. AWS manages the OS, the application server, and the other infrastructure components so that you can focus on developing your application code.

Best practices include: 

• Evaluate the available compute options
<br>
• Understand the available compute configuration options 
<br>
• Collect computer-related metrics 
<br>
• Use the available elasticity of resources 
<br>
• Re-evaluate compute needs based on metrics
<br>
<br>
-**Amazon EC2** 
Running servers on-premises is an expensive undertaking. Amazon Elastic Compute Cloud (Amazon EC2) provides virtual machines where you can host the same kinds of applications that you might run on a traditional on-premises server. Some common uses for EC2 instances include, but are not limited to: Application servers, Web servers, Database servers, Game servers, Mail servers, File servers, Computing servers, and Proxy servers.

The EC2 in Amazon EC2 stands for Elastic Compute Cloud: 

• Elastic refers to the fact that you can easily increase or decrease the number of servers
you run to support an application automatically, and you can also increase or decrease the size of existing servers.
<br>
• Compute refers to reason why most users run servers in the first place, which is to host running applications or process data—actions that require compute resources, including processing power (CPU) and memory (RAM).
<br>
• Cloud refers to the fact that the EC2 instances that you run are hosted in the cloud.

Instances launch from Amazon Machine Images (AMIs), which are effectively virtual machine templates. You can control traffic to and from instances by using security groups. The Launch Instance Wizard makes it easy to launch an instance. 


An AMI includes the following components: 

• A template for the root volume of the instance. A root volume typically contains an operating system (OS) and everything that was installed in that OS (applications, libraries, etc.). Amazon EC2 copies the template to the root volume of a new EC2 instance, and then starts it.
<br>
• Launch permissions that control which AWS accounts can use the AMI. 
<br>
• A block device mapping that specifies the volumes to attach to the instance (if any) when it is launched.

You can choose many AMIs: 

• Quick Start – AWS offers a number of pre-built AMIs for launching your instances.  
• My AMIs – These AMIs are AMIs that you created.
<br>
• AWS Marketplace – The AWS Marketplace offers a digital catalog that lists thousands of software solutions. 
<br>
• Community AMIs – These AMIs are created by people all around the world. These AMIs are not checked by AWS, so use them at your own risk. 

When you create an AMI, Amazon EC2 stops the instance, creates a snapshot of its root volume, and finally registers the snapshot as an AMI. After an AMI is registered, the AMI can be used to launch new instances in the same AWS Region. 


Amazon EC2 provides a selection of instance types that optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity. Instance type categories include general purpose, compute optimized, memory optimized, storage optimized, and accelerated computing instances.

When you look at an EC2 instance type, you will see that its name has several parts. For example, consider the T type. T is the family name, which is then followed by a number.  The number is the generation number of that type. So, a t3 instance is the third generation of the T family. In general, instance types that are of a higher generation are more powerful and provide a better value for the price. The next part of the name is the size portion of the instance. When you compare sizes, it is important to look at the coefficient portion of the size category.
For example, a t3.2xlarge has twice the vCPU and memory of a t3.xlarge. It is also important to note that network bandwidth is also tied to the size of the Amazon EC2 instance. If you will run jobs that will be very network-intensive, you might be required to increase the instance specifications to meet your needs.

Instance types vary in several ways, including: CPU type, CPU or core count, storage type, storage amount, memory amount, and network performance. T3 instances are general purpose instances that provide a baseline level of CPU performance with the ability to burst above the baseline. Use cases for this type of instance include websites and web applications, development environments. C5 instances are optimized for compute-intensive workloads, and deliver cost-effective high performance at a low price per compute ratio. Use cases include scientific modeling, batch processing. R5 instances are optimized for memory-intensive applications. Use cases include high-performance databases, data mining and analysis.

It is also important to consider your network bandwidth requirements. Each instance type provides a documented network performance level. When you launch multiple new EC2 instances, Amazon EC2 attempts to place the instances so that they are spread out across the underlying hardware by default. It does this to minimize correlated failures. However, if you want to specify specific placement criteria, you can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. 

After you have choose an AMI and an instance type, you must specify the network location where the EC2 instance will be deployed. The choice of Region must be made before you start the Launch Instance Wizard. When you launch an instance in a default VPC, AWS will assign it a public IP address by default. When you launch an instance into a nondefault VPC, the subnet has an attribute that determines whether instances launched into that subnet receive a public IP address from the public IPv4 address pool. By default, AWS will not assign a public IP address to instances that are launched in a nondefault subnet. 

It is common to use EC2 instances to run an application that must make secure API calls to other AWS services. To support these use cases, AWS enables you to attach an AWS Identity and Access Management (IAM) role to an EC2 instance. You should never store AWS credentials on an EC2 instance. An instance profile is a container for an IAM role. You can attach an IAM role when you launch the instance and also attach a role to an already running EC2 instance. 

When you create your EC2 instances, you have the option of passing user data to the instance. User data can automate the completion of installations and configurations at instance launch.

When you launch an EC2 instance, you can configure storage options. For each volume that your instance will have, you can specify the size of the disks, the volume types, and whether the storage will be retained if the instance is terminated. You can also specify if encryption should be used.

Amazon Elastic Block Store (Amazon EBS) is an easy-to-use, high-performance durable block storage service that is designed to be used with Amazon EC2 for both throughput-and transaction-intensive workloads. Only instances that are backed by Amazon EBS can be stopped. 

Amazon EC2 Instance Store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance Store works well when you must temporarily store information that changes frequently, such as buffers, caches, scratch data, and other temporary content. You can also use Instance Store for data that is replicated across a fleet of instances, such as a load balanced pool of web servers.

Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic Network File System (NFS) file system for use with AWS Cloud services and on-premises resources. 

Amazon Simple Storage Service (Amazon S3) is an object storage service that offers scalability, data availability, security, and performance. 

A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags enable you to categorize AWS resources, such as EC2 instances, in different ways. Tag keys and tag values are case-sensitive.

A security group acts as a virtual firewall that controls network traffic for one or more instances. You can add rules to each security group. Rules allow traffic to or from its associated instances. When AWS decides whether to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated. When you define a rule, you can specify the allowable source of the network communication (inbound rules) or destination (outbound rules). The source can be an IP address, an IP address range, another security group, a gateway VPC endpoint, or anywhere (which means that all sources will be allowed). Network access control lists (network ACLs) can also be used are firewalls to protect subnets in a VPC.

Amazon EC2 uses public–key cryptography to encrypt and decrypt login information. The technology uses a public key to encrypt a piece of data, and then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. 

An instance can be in one of the following states: Pending, Running, Rebooting, Shutting down, Terminated, Stopping, Stopped.

A public IP address is an IPv4 address that is reachable from the internet. Each instance that receives a public IP address is also given an external DNS hostname. If you require a persistent public IP address, you might want to associate an Elastic IP address with the instance. To associate an Elastic IP address, you must first allocate a new Elastic IP address in the Region where the instance exists. By default, all AWS accounts are limited to five (5) Elastic IP addresses per Region because public (IPv4) internet addresses are a scarce public resource. However, this is a soft limit.

Instance metadata is data about your instance. You can view it while you are connected to the instance. To access it in a browser, go to the following URL: http://169.254.169.254/latest/meta-data/. The data can also be read programmatically, such as from a terminal window that has the cURL utility. In the terminal window, run curl http://169.254.169.254/latest/meta-data/. Any user data that is specified at instance launch can also be accessed at the following URL: http://169.254.169.254/latest/user-data. 

You can monitor your instances by using Amazon CloudWatch, which collects and processes raw data from Amazon EC2 into readable, near-real-time metrics. These statistics are recorded for a period of 15 months, so you can access historical information and gain a better perspective on how your web application or service is performing. By default, Amazon EC2 provides basic monitoring, which sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance. 
<br>
<br>
-**Amazon EC2 versus a managed service like Amazon Relational Database Service**
With Amazon RDS for SQL Server, you can meet a number of requirements and improve security, database performance, scalability, and ease of management.

Improve Manageability for SQL Server with Pre-configured Parameters, Upgrade Monitoring and Metrics, Receive DB Event Notifications, Enable Automatic Software Patching, Scale in a Few Clicks, Improve Backup and Disaster Recovery.
<br>
<br>
-**Amazon EC2 Cost Optimization**

Amazon offers different pricing models to choose from when you want to run EC2 instances:

Per second billing is only available for On-Demand Instances, Reserved Instances, and Spot Instances that run Amazon Linux or Ubuntu.
On-Demand Instances are eligible for the AWS Free Tier. They have the lowest upfront cost and the most flexibility. 
Dedicated Instances are instances that run in a virtual private cloud (VPC) on hardware that’s dedicated to a single customer. 
Reserved Instance enable you to reserve computing capacity for 1-year or 3-year term with lower hourly running costs. 
Scheduled Reserved Instances enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified duration, for a 1-year term. You pay for the time that the instances are scheduled, even if you do not use them.
Spot Instances enable you to bid on unused EC2 instances, which can lower your costs. The hourly price for a Spot Instance fluctuates depending on supply and demand. Your Spot Instance runs whenever your bid exceeds the current market price.


Each Amazon EC2 pricing model provides a different set of benefits: 

On-Demand Instances offer the most flexibility, with no long-term contract and low rates. Works well for spiky workloads or if you only need to test or run an application for a short time (for example, during application development or testing). Sometimes, your workloads are unpredictable, and On-Demand Instances are a good choice for these cases.
Spot Instances provide large scale at a significantly discounted price. Spot Instances are a good choice if your applications can tolerate interruption with a 2-minute warning notification. By default, instances are terminated, but you can configure them to stop or hibernate instead. Common use cases include fault-tolerant applications such as web servers, API backends, and big data processing. 
Reserved Instances are a good choice if you have predictable or steady-state compute needs (for example, an instance that you know you want to keep running most or all of the time for months or years).
Dedicated Hosts are a good choice when you have licensing restrictions for the software you want to run on Amazon EC2, or when you have specific compliance or regulatory requirements that preclude you from using the other deployment options.

To optimize costs, you must consider four consistent, powerful drivers:

• Right-size – Choose the right balance of instance types. Notice when servers can be either sized down or turned off, and still meet your performance requirements. AWS offers approximately 60 instance types and sizes. Right-sizing is the process of reviewing deployed resources and looking for opportunities to downsize when possible. To right size: Select the cheapest instance available that still meets your performance requirements. - Review CPU, RAM, storage, and network utilization to identify instances that could be downsized. For right-sizing, use techniques such as load testing to your advantage. - Use Amazon CloudWatch metrics and set up custom metrics.
<br>
• Increase elasticity – Design your deployments to reduce the amount of server capacity that is idle by implementing deployments that are elastic, such as deployments that use automatic scaling to handle peak loads. One form of elasticity is to create, start, or use EC2 instances when they are needed, but then to turn them off when they are not in use. Elasticity is one of the central tenets of the cloud, but customers often go through a learning process to operationalize elasticity to drive cost savings. As a rule of thumb, you should target 20–30 percent of your Amazon EC2 instances to run as On-Demand Instances or Spot Instances. 
<br>
• Optimal pricing model – Recognize the available pricing options. Analyze your usage patterns so that you can run EC2 instances with the right mix of pricing options. Consider the application architecture. Does the functionality provided by your application need to run on an EC2 virtual machine? Perhaps by making use of the AWS Lambda service instead.
<br>
• Optimize storage choices – Analyze the storage requirements of your deployments. Reduce unused storage overhead when possible, and choose less expensive storage options if they can still meet your requirements for storage performance. One way you can accomplish this is by resizing EBS volumes. There are also a variety of EBS volume types. Choose the least expensive type that still meets your performance requirements. Delete these unneeded EBS snapshots. Try to identify the most appropriate destination for specific types of data.

Tagging helps provide information about what resources are being used by whom and for what purpose. You can activate cost allocation tags in the Billing and Cost Management console, and AWS can generate a cost allocation report with usage and costs grouped by your active tags. Use AWS services such as AWS Trusted Advisor. Cost-optimization efforts are typically more successful when the responsibility for cost optimization is assigned to an individual or to a team.
<br>
<br>
-**Container Services**
Containers are a method of operating system virtualization that enables you to run an application and its dependencies in resource-isolated processes. By using containers, you can easily package an application's code, configurations, and dependencies into easy-to-use building blocks that deliver environmental consistency, operational efficiency, developer productivity, and version control. Containers are smaller than virtual machines, and do not contain an entire operating system. Instead, containers share a virtualized operating system and run as resource-isolated processes, which ensure quick, reliable, and consistent deployments. Containers deliver environmental consistency because the application’s code, configurations, and dependencies are packaged into a single object. In terms of space, container images are usually an order of magnitude smaller than virtual machines.

Docker is a software platform that packages software (such as applications) into containers. By using Docker, you can quickly deploy and scale applications into any environment. Docker is best used as a solution when you want to: Standardize environments, Reduce conflicts between language stacks and versions, Use containers as a service, Run microservices using standardized code deployments, and Require portability for data processing. 

One significant difference between containers and vm's is that virtual machines run directly on a hypervisor, but containers can run on any Linux OS if they have the appropriate kernel feature support and the Docker daemon is present. This makes containers very portable. 

You could launch one or more Amazon EC2 instances, install Docker on each instance, and manage and run the Docker containers on those Amazon EC2 instances yourself. While that is an option, AWS provides a service called Amazon Elastic Container Service (Amazon ECS) that simplifies container management. Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container management service that supports Docker containers. Amazon ECS enables you to easily run applications on a managed cluster of Amazon EC2 instances. Essential Amazon ECS features include the ability to: Launch up to tens of thousands of Docker containers in seconds, Monitor container deployment, Manage the state of the cluster that runs the containers, Schedule containers by using a built-in scheduler or a third-party scheduler (for example, Apache Mesos or Blox). Amazon ECS clusters can also use Spot Instances and Reserved Instances. To prepare your application to run on Amazon ECS, you create a task definition which is a text file that describes one or more containers, up to a maximum of ten, that form your application. It can be thought of as a blueprint for your application. A task is the instantiation of a task definition within a cluster. You can specify the number of tasks that will run on your cluster. The Amazon ECS task scheduler is responsible for placing tasks within your cluster. When Amazon ECS runs the containers that make up your task, it places them on an ECS cluster. The cluster (when you choose the EC2 launch type) consists of a group of EC2 instances each of which is running an Amazon ECS container agent.

When you create an Amazon ECS cluster, you have three options: A Networking Only cluster (powered by AWS Fargate), An EC2 Linux + Networking cluster, An EC2 Windows + Networking cluster. If you choose one of the two EC2 launch type options, you will then be prompted to choose whether the cluster EC2 instances will run as On-Demand Instances or Spot Instances. In addition, you will need to specify many details about the EC2 instances that will make up your cluster. Amazon ECS keeps track of all the CPU, memory, and other resources in your cluster. 
If you choose the networking-only Fargate launch type, then the cluster that will run your containers will be managed by AWS. The Fargate option enables you to focus on designing and building your applications.

Kubernetes is open source software for container orchestration. Kubernetes can work with many containerization technologies, including Docker. Kubernetes enables you to deploy and manage containerized applications at scale. With Kubernetes, you can run any type of containerized application by using the same toolset in both on-premises data centers and the cloud. Kubernetes manages a cluster of compute instances (called nodes). It runs containers on the cluster, which are based on where compute resources are available and the resource requirements of each container. Containers are run in logical groupings called pods. A key advantage of Kubernetes is that you can use it to run your containerized applications anywhere without needing to change your operational tooling. 

You could launch one or more Amazon EC2 instances, install Docker on each instance, install Kubernetes on the cluster, and manage and run Kubernetes yourself. While that is an option, AWS provides a service called Amazon Elastic Kubernetes Service (Amazon EKS) that simplifies the management of Kubernetes clusters. Amazon Elastic Kubernetes Service (Amazon EKS) is a managed Kubernetes service that makes it easy for you to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane. Amazon offers both Amazon ECS and Amazon EKS (they are both capable of orchestrating Docker containers) to provide customers with flexible options. 


Amazon Elastic Container Registry (Amazon ECR) is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. It is integrated with Amazon ECS, so you can store, run, and manage container images for applications that run on Amazon ECS. Specify the Amazon ECR repository in your task definition, and Amazon ECS will retrieve the appropriate images for your applications. You can transfer your container images to and from Amazon ECS via HTTPS. Your images are also automatically encrypted at rest using Amazon S3 server-side encryption.

Some key takeaways from this section include: Containers can hold everything that an application needs to run. Docker is a software platform that packages software into containers. A single application can span multiple containers. Amazon Elastic Container Service (Amazon ECS) orchestrates the execution of Docker containers. Kubernetes is open source software for container orchestration. Amazon Elastic Kubernetes Service (Amazon EKS) enables you to run Kubernetes on AWS. Amazon Elastic Container Registry (Amazon ECR) enables you to store, manage, and deploy your Docker containers.





<br>
-Security groups, VPC sharing participants can reference the security group IDs of each other
<br>
-Efficiencies, higher density in subnets, efficient use of VPNs and AWS Direct Connect
<br>
-No hard limits, hard limits can be avoided
<br>
-Optimized costs, costs can be optimized through the reuse of NAT gateways, VPC interface endpoints, and intra-Availability Zone traffic

A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.

By default, instances that you launch into a VPC cannot communicate with a remote network. To connect your VPC to your remote network (that is, create a virtual private network or VPN connection), you: 
<br>
<br>
-Create a new virtual gateway device (called a virtual private network (VPN) gateway) and attach it to your VPC.
<br>
-Define the configuration of the VPN device or the customer gateway. 
<br>
Create a custom route table to point corporate data center-bound traffic to the VPN gateway. You also must update security group rules. 
<br>
-Establish an AWS Site-to-Site VPN (Site-to-Site VPN) connection to link the two systems together.
<br>

One of the challenges of network communication is network performance. Performance can be negatively affected if your data center is located far away from your AWS Region. For such situations, AWS offers AWS Direct Connect, or DX. AWS Direct Connect enables you to establish a dedicated, private network connection between your network and one of the DX locations. 

A VPC endpoint is a virtual device that enables you to privately connect your VPC to supported AWS services and VPC endpoint services that are powered by AWS PrivateLink. Traffic between your VPC and the other service does not leave the Amazon network.
There are two types of VPC endpoints: 1 An interface VPC endpoint (interface endpoint), enables you to connect to services that are powered by AWS PrivateLink. You are charged for creating and using an interface endpoint to a service. Hourly usage rates and data processing rates apply. 2 Gateway endpoints, the use of gateway endpoints incurs no additional charge. 

You can configure your VPCs in several ways, and take advantage of numerous connectivity options and gateways. Things get more complex when customers start to set up connectivity between their VPCs. To solve this problem, you can use AWS Transit Gateway to simplify your networking model. With AWS Transit Gateway, you only need to create and manage a single connection from the central gateway into each VPC, on-premises data center, or remote office across your network.  This ease of connectivity makes it easier to scale your network as you grow.
<br>
<br>
-**VPC Security** A security group acts as a virtual firewall for your instance, and it controls inbound and outbound traffic. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC can be assigned to a different set of security groups. A security group is a way for you to filter traffic to your instances. Security groups have rules that control the inbound and outbound traffic. Security groups are stateful, which means that state information is kept even after a request is processed. When you create a custom security group, you can specify allow rules, but not deny rules. All rules are evaluated before the decision to allow traffic.

A network access control list (network ACL) is an optional layer of security for your Amazon VPC. It acts as a firewall for controlling traffic in and out of one or more subnets. To add another layer of security to your VPC, you can set up network ACLs with rules that are similar to your security groups. Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL. You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed. A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. Network ACLs are stateless, which means that no information about a request is maintained after a request is processed. You can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until you add rules. A network ACL contains a numbered list of rules that are evaluated in order, starting with the lowest numbered rule. The highest number that you can use for a rule is 32,766. AWS recommends that you create rules in increments so that you can insert new rules where you need them later.

Summary of the differences between security groups and network ACLs:
<br>
<br>
-Security groups act at the instance level, but network ACLs act at the subnet level.
<br>
-Security groups support allow rules only, but network ACLs support both allow and deny rules.
<br>
-Security groups are stateful, but network ACLs are stateless.
<br>
-For security groups, all rules are evaluated before the decision is made to allow traffic. For network ACLs, rules are evaluated in number order before the decision is made to allow traffic.
<br>
<br>
-**Route 53** Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS. You can use Amazon Route 53 to configure DNS health checks so you that can route traffic to healthy endpoints or independently monitor the health of your application and its endpoints. Amazon Route 53 traffic flow helps you manage traffic globally through several routing types, which can be combined with DNS failover to enable various low-latency, fault-tolerant architectures. Amazon Route 53 also offers Domain Name Registration. When a user initiates a DNS request, the DNS resolver checks with your domain in Route 53, gets the IP address, and returns it to the user. Amazon Route 53 supports several types of routing policies, which determine how Amazon Route 53 responds to queries:
<br>
<br>
-Simple routing (round robin), use for a single resource that performs a given function for your domain (such as a web server that serves content for the example.com website).
<br>
-Weighted round robin routing, use to route traffic to multiple resources in proportions that you specify. Enables you to assign weights to resource record sets to specify the frequency with which different responses are served. You might want to use this capability to do A/B testing, which is when you send a small portion of traffic to a server where you made a software change. <br>
-Latency routing (LBR), use when you have resources in multiple AWS Regions and you want to route traffic to the Region that provides the best latency. Latency routing works by routing your customers to the AWS endpoint that provides the fastest experience based on actual performance measurements of the different AWS Regions where your application runs.
<br>
-Geolocation routing, use when you want to route traffic based on the location of your users. When you use geolocation routing, you can localize your content and present some or all of your website in the language of your users.
<br>
-Geoproximity routing, use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.
-Failover routing (DNS failover), use when you want to configure active-passive failover. Amazon Route 53 can help detect an outage of your website and redirect your users to alternate locations where your application is operating properly. 
<br>
-Multivalue answer routing, use when you want Route 53 to respond to DNS queries with up to eight healthy records that are selected at random. You can specify multiple values for almost any record, but multivalue answer routing also enables you to check the health of each resource so that Route 53 returns only values for healthy resources.

Multi-Region deployment is an example use case for Amazon Route 53. With Amazon Route 53, the user is automatically directed to the Elastic Load Balancing load balancer that’s closest to the user.

Amazon Route 53 enables you to improve the availability of your applications that run on AWS by:
<br>
<br>
-Configuring backup and failover scenarios for your own applications.
<br>
-Enabling highly available multi-Region architectures on AWS.
<br>
-Creating health checks to monitor the health and performance of your web applications, web servers, and other resources.
<br>
<br>
-**CloudFront** The number of network hops and the distance that requests must travel significantly affect the performance and responsiveness of websites. Further, network latency is different in various geographic locations. For these reasons, a content delivery network might be the solution.

A content delivery network (CDN) is a globally distributed system of caching servers. A CDN caches copies of commonly requested files that are hosted on the application origin server. The CDN delivers a local copy of the requested content from a cache edge or Point of Presence that provides the fastest delivery to the requester. CDNs also deliver dynamic content that is unique to the requester and is not cacheable. Having a CDN deliver dynamic content improves application performance and scaling. Also, content such as form data, images, and text can be ingested and sent back to the origin, thus taking advantage of the low-latency connections and proxy behavior of the PoP.

Amazon CloudFront is a fast CDN service that securely delivers data, videos, applications, and application programming interfaces (APIs) to customers globally with low latency and high transfer speeds. Amazon CloudFront delivers files to users over a global network of edge locations and Regional edge caches (for the less popular content). 

Amazon CloudFront provides the following benefits:
<br>
<br>
-Fast and global 
<br>
-Security at the edge
<br>
-Highly programmable 
<br>
-Deeply integrated with AWS 
<br>
-Cost-effective 

Amazon CloudFront charges are based on actual usage of the service in four areas: 
<br>
<br>
-Data transfer out 
<br>
-HTTP(S) requests 
<br>
-Invalidation requests 
<br>
-Dedicated IP custom Secure Sockets Layer (SSL)
<br>
<br>


---

### Lab 6: Scale and Load Balance Your Architecture

Lab 6 illustrates the use of Elastic Load Balancing (ELB) and Auto Scaling to load balance as well as automatically scale infrastructure. Elastic Load Balancing automatically distributes application traffic across Amazon EC2 instances. Auto Scaling helps to maintain application availability and allows you to scale your Amazon EC2 capacity automatically according to conditions you define.

#### Topics

-Create an Amazon Machine Image (AMI) from a running instance
<br>
-Create a load balancer
<br>
-Create a launch configuration and an Auto Scaling group
<br>
-Automatically scale new instances within a private subnet 
<br>
-Create Amazon CloudWatch alarms and monitor performance of your infrastructure

#### Tasks

**Create an AMI for Auto Scaling**
<br>
-In the AWS Management Console, on the Services menu, click EC2.
<br>
-In the left navigation pane, click Instances.
<br>
-In the Actions menu, click Image > Create Image and configure.
<br>
**Create a Load Balancer**
<br>
-In the left navigation pane, click Load Balancers. Click Create Load Balancer. Select Application Load Balancer and click Create. Then configure.
<br>
**Create a Launch Configuration and an Auto Scaling Group**
<br>
-In the left navigation pane, click Launch Configurations. Click Create launch configuration.
<br>
-In the left navigation pane, click My AMIs. In the row for Web Server AMI, click Select.
<br>
-Select the t2.micro instance type and click Next: Configure details. Configure settings. 
<br>
-Click Next: Add Storage. Click Next: Configure Security Group. Configure settings.
<br>
-Review the details of your launch configuration and click Create launch configuration.
<br>
-Click Create an Auto Scaling group using this launch configuration. Configure settings.
<br>
-Expand Advanced Details, then configure. Select Receive traffic from one or more load balancers. Select Enable CloudWatch detailed monitoring.
<br>
-Click Next: Configure scaling policies. Select Use scaling policies to adjust the capacity of this group. Modify the Scale. In Scale Group Size, Metric type: Average CPU Utilization. Target value: 60. This tells Auto Scaling to maintain an average CPU utilization across all instances at 60%. 
<br>
-Click Next: Configure Notifications. 
<br>
-Click Next: Configure Tags. 
<br>
-Click Review then click Create Auto Scaling group.
<br>
**Verify that Load Balancing is Working**
<br>
-In the left navigation pane, click Instances. You should see two new instances named Lab Instance launched by Auto Scaling.
<br>
-In the left navigation pane, click Target Groups. Click the Targets tab. Two Lab Instance targets should be listed for this target group. Wait until the Status of both instances transitions to healthy.
<br>
-In the left navigation pane, click Load Balancers. Copy the DNS name of the load balancer.
<br>
-Open a new web browser tab, paste the DNS Name you just copied, and press Enter. The application should appear in your browser indicating that the Load Balancer received the request, sent it to one of the EC2 instances, then passed back the result.
<br>
**Test Auto Scaling**
<br>
-In the AWS management console, on the Services menu, click CloudWatch.
<br>
-In the left navigation pane, click Alarms.
<br>
-Switch to the application browser tab and Click Load Test beside the AWS logo.
<br>
-Return to browser tab with the CloudWatch console. The AlarmLow alarm should change to OK and the AlarmHigh alarm status should change to ALARM.
<br>
-You can now view the additional instance(s) that were launched. On the Services menu, click EC2. In the left navigation pane, click Instances. More than two instances labeled Lab Instance should now be running.
<br>
**Terminate Web Server 1**
<br>
-Select Web Server 1. 
<br>
-In the Actions menu, click Instance State > Terminate.
<br>
<br>

---

### Activity: AWS Elastic Beanstalk

For this activity we deployed code to an AWS Elastic Beanstalk environment and observed the resources.

#### Tasks

**Access the Elastic Beanstalk environment**
<br>
-In the AWS Management Console, on the Services menu, choose Elastic Beanstalk.
<br>
-In the green application details box, click the URL value that displays.
<br>
-Near the top of the page, click the URL (the URL ends in elasticbeanstalk.com). A new browser tab opens. Return to the Elastic Beanstalk console.
<br>
**Deploy a sample application to Elastic Beanstalk** 
<br>
-Download a sample application. Back in the Elastic Beanstalk console, click Upload and Deploy.
<br>
-After the deployment is complete, click the URL value near the top of the screen to display the deployed web application.
<br>
-Back in the Elastic Beanstalk console, click Configuration.
<br>
-Click Monitoring. Browse through the charts to see the kinds of information that are available to you.
<br>
**Explore the AWS resources that support your application**
<br>
-From the Services menu, choose EC2, click Instances.
<br>
-There are two instances running that support your web application.
<br>




[back](../blog.html)
