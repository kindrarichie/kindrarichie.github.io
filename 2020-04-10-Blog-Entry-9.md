---
layout: default
---

## Blog Post 8

This week the course will again be online only due to COVID-19. 

I completed the AWS training for module 7. 

---

### AWS Module 7: Storage

Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Cloud storage is a critical component of cloud computing because it holds the information that applications use. Some broad categories of storage include:
<br>

• Instance store, or ephemeral storage, is temporary storage that is added to your Amazon EC2 instance.
<br>
• Amazon EBS is persistent, mountable storage that can be mounted as a device to an Amazon EC2 instance. Amazon EBS can be mounted to an Amazon EC2 instance only within the same Availability Zone. Only one Amazon EC2 instance at a time can mount an Amazon EBS volume.
<br>
• Amazon EFS is a shared file system that multiple Amazon EC2 instances can mount at the same time.
<br>
• Amazon S3 is persistent storage where each file becomes an object and is available through a Uniform Resource Locator (URL); it can be accessed from anywhere.
<br>
• Amazon S3 Glacier is for cold storage for data that is not accessed frequently (for example, when you need long-term data storage for archival or compliance reasons).
<br>


#### Topics

-**Amazon EBS (Elastic Block Store)** 

Amazon EBS provides persistent block storage volumes for use with Amazon EC2 instances. Persistent storage is any data storage device that retains data after power to that device is shut off. It is also sometimes called non-volatile storage. Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure. It is designed for high availability and durability. Amazon EBS volumes provide the consistent and low-latency performance. You can scale your usage up or down within minutes. 

One critical difference between some storage types is whether they offer block-level storage or object-level storage. This difference has a major effect on the throughput, latency, and cost of your storage solution. Block storage solutions are typically faster and use less bandwidth, but they can cost more than object-level storage.

Amazon EBS enables you to create individual storage volumes and attach them to an Amazon EC2 instance. Because they are directly attached to the instances, they can provide low latency between where the data is stored and where it might be used on the instance. For this reason, they can be used to run a database with an Amazon EC2 instance. Amazon EBS volumes are included as part of the backup of your instances into Amazon Machine Images (or AMIs). AMIs are stored in Amazon S3 and can be reused to create new Amazon EC2 instances later.
A backup of an Amazon EBS volume is called a snapshot. The first snapshot is called the baseline snapshot. Any other snapshot after the baseline captures only what is different from the previous snapshot. 

Matching the correct technology to your workload is a best practice for reducing storage costs. Provisioned IOPS SSD-backed Amazon EBS volumes can give you the highest performance. Only SSDs can be used as boot volumes for EC2 instances. 

You can use Amazon EBS volumes as primary storage for data that requires frequent updates. You can also use them for throughput-intensive applications that perform continuous disk scans. Amazon EBS volumes persist independently from the running life of an EC2 instance. Use cases for EBS vary by the storage type used and whether you are using General Purpose or Provisioned IOPS. 

To provide an even higher level of data durability, Amazon EBS enables you to create point-in-time snapshots of your volumes, and you can re-create a new volume from a snapshot at any time. You can also share snapshots or even copy snapshots to different AWS Regions for even greater disaster recovery (DR) protection. You can also have encrypted Amazon EBS volumes at no additional cost. 

To provide an even higher level of data durability, Amazon EBS enables you to create point-in-time snapshots of your volumes, and you can re-create a new volume from a snapshot at any time. You can also share snapshots or even copy snapshots to different AWS Regions for even greater disaster recovery (DR) protection. For example, you can encrypt and share your snapshots from Virginia in the US to Tokyo, Japan. You can also have encrypted Amazon EBS volumes at no additional cost. Amazon EBS volumes can increase capacity (dynamically) and change to different types, so you can change from hard disk drives (HDDs) to solid state drives (SSDs). 

When you begin to estimate the cost for Amazon EBS, you must consider the following:
<br>
1. Volumes – Volume storage for all Amazon EBS volume types is charged by the amount you provision in GB per month, until you release the storage.
2. IOPS – I/O is included in the price of General Purpose SSD volumes. However, for Amazon EBS magnetic volumes, I/O is charged by the number of requests that you make to your volume. With Provisioned IOPS SSD volumes, you are also charged by the amount you provision in IOPS (multiplied by the percentage of days that you provision for the month).

The pricing and provisioning of Amazon EBS are complex.

3. Snapshots – Amazon EBS enables you to back up snapshots of your data to Amazon S3 for durable recovery. If you opt for Amazon EBS snapshots, the added cost is per GB-month of data stored.
4. Data transfer – When you copy Amazon EBS snapshots, you are charged for the data that is transferred across Regions. After the snapshot is copied, standard Amazon EBS snapshot charges apply for storage in the destination Region.

Section takeaways:
<br>

Amazon EBS provides block-level storage volumes for use with Amazon EC2 instances. Amazon EBS volumes are off-instance storage that persists independently from the life of an instance. They are analogous to virtual disks in the cloud. Amazon EBS provides three volume types: General Purpose SSD, Provisioned IOPS SSD, and magnetic.
The three volume types differ in performance characteristics and cost, so you can choose the right storage performance and price for the needs of your applications.
Additional benefits include replication in the same Availability Zone, easy and transparent encryption, elastic volumes, and backup by using snapshots.
<br>
<br>
-**Amazon Simple Storage Service** 
Amazon S3 is object storage (if you want to change a part of a file, you must make the change and then re-upload the entire modified file) that is built to store and retrieve any amount of data from anywhere.

Amazon S3 stores data as objects within resources that are called buckets.

Amazon S3 is a managed cloud storage solution that is designed to scale seamlessly and provide 11 9s of durability. You can store virtually as many objects as you want in a bucket. Objects can be up to 5 TB in size. By default, data in Amazon S3 is stored redundantly across multiple facilities and multiple devices in each facility. Because Amazon S3 supports objects as large as several terabytes in size, you can even store database snapshots as objects. Amazon S3 also provides low-latency access to the data over the internet by Hypertext Transfer Protocol (HTTP) or Secure HTTP (HTTPS), so you can retrieve data anytime from anywhere. You can also access Amazon S3 privately through a virtual private cloud (VPC) endpoint. You get fine-grained control over who can access your data by using AWS Identity and Access Management (IAM) policies, Amazon S3 bucket policies, and even per-object access control lists.

By default, none of your data is shared publicly. You can also encrypt your data in transit and choose to enable server-side encryption on your objects. You can access Amazon S3 through the web-based AWS Management Console; programmatically through the API and SDKs; or with third-party solutions, which use the API or the SDKs.
Amazon S3 includes event notifications that enable you to set up automatic notifications when certain events occur. Those notifications can be sent to you, or they can be used to trigger other processes, such as AWS Lambda functions. With storage class analysis, you can analyze storage access patterns and transition the right data to the right storage class. The Amazon S3 Analytics feature automatically identifies the optimal lifecycle policy to transition less frequently accessed storage to Amazon S3 Standard – Infrequent Access (Amazon S3 Standard-IA). 

Amazon S3 offers a range of object-level storage classes that are designed for different use cases. These classes include:
<br>
• Amazon S3 Standard – Amazon S3 Standard is designed for high durability, availability, and performance object storage for frequently accessed data. It delivers low latency and high throughput.
• Amazon S3 Intelligent-Tiering – The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in Amazon S3 Intelligent-Tiering. It works well for long-lived data with access patterns that are unknown or unpredictable.
• Amazon S3 Standard-Infrequent Access (Amazon S3 Standard-IA) – The Amazon S3 Standard-IA storage class is used for data that is accessed less frequently, but requires rapid access when needed. Amazon S3 Standard-IA is designed to provide the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per-GB storage price and per-GB retrieval fee. 
• Amazon S3 One Zone-Infrequent Access (Amazon S3 One Zone-IA) – Amazon S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other Amazon S3 storage classes, which store data in a minimum of three Availability Zones, Amazon S3 One Zone-IA stores data in a single Availability Zone and it costs less than Amazon S3 Standard-IA. 
• Amazon S3 Glacier – Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with—or cheaper than—on-premises solutions. To keep costs low yet suitable for varying needs, Amazon S3 Glacier provides three retrieval options that range from a few minutes to hours. 
• Amazon S3 Glacier Deep Archive – Amazon S3 Glacier Deep Archive is the lowest-cost storage class for Amazon S3. It supports long-term retention and digital preservation for data that might be accessed once or twice in a year. 

Amazon S3 stores data inside buckets. Buckets are essentially the prefix for a set of files, and must be uniquely named across all of Amazon S3 globally. Buckets are logical containers for objects. You can have one or more buckets in your account. You can control access for each bucket—who can create, delete, and list objects in the bucket. You can also view access logs for the bucket and its objects, and choose the geographical region where Amazon S3 stores the bucket and its contents. To upload your data, create a bucket in an AWS Region, and then upload almost any number of objects to the bucket. Amazon S3 refers to files as objects. As soon as you have a bucket, you can store almost any number of objects inside it. An object is composed of data and any metadata that describes that file, including a URL. To store an object in Amazon S3, you upload the file that you want to store to a bucket. When you upload a file, you can set permissions on the data and any metadata.

When you create a bucket in Amazon S3, it is associated with a specific AWS Region. When you store data in the bucket, it is redundantly stored across multiple AWS facilities within your selected Region.

Amazon S3 automatically manages the storage behind your bucket while your data grows. Amazon S3 also scales to handle a high volume of requests. 

You can access Amazon S3 through the console, AWS Command Line Interface (AWS CLI), or AWS SDK. You can also access the data in your bucket directly by using REST-based endpoints.
The endpoints support HTTP or HTTPS access. To support this type of URL-based access, Amazon S3 bucket names must be globally unique and Domain Name Server (DNS)-compliant.
Also, object keys should use characters that are safe for URLs. 

Some use cases for Amazon S3:
<br>
• As a location for any application data, Amazon S3 buckets provide a shared location for storing objects that any instances of your application can access—including applications on Amazon EC2 or even traditional servers. Because the content can be fetched directly over the internet, you can offload serving that content from your application and enable clients to directly fetch the data from Amazon S3 themselves.
• For static web hosting, Amazon S3 buckets can serve the static contents of your website, including HTML, CSS, JavaScript, and other files.
• The high durability of Amazon S3 makes it a good candidate for storing backups of your data. For greater availability and disaster recovery capability, Amazon S3 can even be configured to support cross-Region replication.

Amazon S3 common scenarios:
<br>
Backup and storage – Provide data backup and storage services for others 
Application hosting – Provide services that deploy, install, and manage web applications
Media hosting – Build a redundant, scalable, and highly available infrastructure that hosts video, photo, or music uploads and downloads
Software delivery – Host your software applications that customers can download

With Amazon S3, specific costs vary depending on the Region and the specific requests that were made. As a general rule, you pay only for transfers that cross the boundary of your Region, which means you do not pay for transfers in to Amazon S3 or transfers out from Amazon S3 to Amazon CloudFront edge locations within that same Region.

When you begin to estimate the costs of Amazon S3, you must consider the following:
<br>
1. Storage class type – • Standard storage is designed to provide 11 9s of durability and four 9s of availability.
• S3 Standard – Infrequent Access (S-IA) is a storage option within Amazon S3 that you can use to reduce your costs by storing less frequently accessed data at slightly lower levels of redundancy than Amazon S3 standard storage. 
2. Amount of storage – The number and size of objects stored in your Amazon S3 buckets.
3. Requests – Consider the number and type of requests. GET requests incur charges at different rates than other requests, such as PUT and COPY requests. • GET – Retrieves an object from Amazon S3. You must have READ access to use this operation.
• PUT – Adds an object to a bucket. You must have WRITE permissions on a bucket to add an object to it.
• COPY – Creates a copy of an object that is already stored in Amazon S3. A COPY operation is the same as performing a GET and then a PUT.
4. Data transfer – Consider the amount of data that is transferred out of the Amazon S3 Region. Remember that data transfer in is free, but there is a charge for data transfer out.
<br>
<br>

-**Amazon Elastic File System (Amazon EFS)** Amazon EFS implements storage for EC2 instances that multiple virtual machines can access at the same time. It is implemented as a shared file system that uses the Network File System (NFS) protocol.


• Amazon Elastic Container Registry (Amazon ECR) is used to store and retrieve Docker images.
<br>
• Amazon Elastic Container Service (Amazon ECS) is a container orchestration service that supports Docker.
<br>
• VMware Cloud on AWS enables you to provision a hybrid cloud without custom hardware.
<br>
• AWS Elastic Beanstalk provides a simple way to run and manage web applications. 
<br>
• AWS Lambda is a serverless compute solution. You pay only for the compute time that you use.
<br>
• Amazon Elastic Kubernetes Service (Amazon EKS) enables you to run managed Kubernetes on AWS.
<br>
• Amazon Lightsail provides a simple-to-use service for building an application or website. • AWS Batch provides a tool for running batch jobs at any scale. 
<br>
• AWS Fargate provides a way to run containers that reduce the need for you to manage servers or clusters.
<br>
• AWS Outposts provides a way to run select AWS services in your on-premises data center. 
<br>
• AWS Serverless Application Repository provides a way to discover, deploy, and publish serverless applications.

Each AWS compute service belongs to one of four broad categories: virtual machines (VMs) that provide infrastructure as a service (IaaS), serverless, container-based, and platform as a service (PaaS).

Amazon EC2 provides virtual machines, and you can think of it as infrastructure as a service (IaaS). IaaS services provide flexibility and leave many of the server management responsibilities to you. 

AWS Lambda is a zero-administration compute platform. AWS Lambda enables you to run code without provisioning or managing servers. You pay only for the compute time that is consumed. 

Container-based services—including Amazon Elastic Container Service, Amazon Elastic Kubernetes Service, AWS Fargate, and Amazon Elastic Container Registry—enable you to run multiple workloads on a single operating system (OS). Containers spin up more quickly than virtual machines, thus offering responsiveness.

AWS Elastic Beanstalk provides a platform as a service (PaaS). It facilitates the quick deployment of applications that you create by providing all the application services that you need. AWS manages the OS, the application server, and the other infrastructure components so that you can focus on developing your application code.

Best practices include: 

• Evaluate the available compute options
<br>
• Understand the available compute configuration options 
<br>
• Collect computer-related metrics 
<br>
• Use the available elasticity of resources 
<br>
• Re-evaluate compute needs based on metrics
<br>
<br>
-**Amazon Elastic File System (Amazon EFS)** 
Running servers on-premises is an expensive undertaking. Amazon Elastic Compute Cloud (Amazon EC2) provides virtual machines where you can host the same kinds of applications that you might run on a traditional on-premises server. Some common uses for EC2 instances include, but are not limited to: Application servers, Web servers, Database servers, Game servers, Mail servers, File servers, Computing servers, and Proxy servers.

The EC2 in Amazon EC2 stands for Elastic Compute Cloud: 

• Elastic refers to the fact that you can easily increase or decrease the number of servers
you run to support an application automatically, and you can also increase or decrease the size of existing servers.
<br>
• Compute refers to reason why most users run servers in the first place, which is to host running applications or process data—actions that require compute resources, including processing power (CPU) and memory (RAM).
<br>
• Cloud refers to the fact that the EC2 instances that you run are hosted in the cloud.

Instances launch from Amazon Machine Images (AMIs), which are effectively virtual machine templates. You can control traffic to and from instances by using security groups. The Launch Instance Wizard makes it easy to launch an instance. 


An AMI includes the following components: 

• A template for the root volume of the instance. A root volume typically contains an operating system (OS) and everything that was installed in that OS (applications, libraries, etc.). Amazon EC2 copies the template to the root volume of a new EC2 instance, and then starts it.
<br>
• Launch permissions that control which AWS accounts can use the AMI. 
<br>
• A block device mapping that specifies the volumes to attach to the instance (if any) when it is launched.

You can choose many AMIs: 

• Quick Start – AWS offers a number of pre-built AMIs for launching your instances.  
• My AMIs – These AMIs are AMIs that you created.
<br>
• AWS Marketplace – The AWS Marketplace offers a digital catalog that lists thousands of software solutions. 
<br>
• Community AMIs – These AMIs are created by people all around the world. These AMIs are not checked by AWS, so use them at your own risk. 

When you create an AMI, Amazon EC2 stops the instance, creates a snapshot of its root volume, and finally registers the snapshot as an AMI. After an AMI is registered, the AMI can be used to launch new instances in the same AWS Region. 


Amazon EC2 provides a selection of instance types that optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity. Instance type categories include general purpose, compute optimized, memory optimized, storage optimized, and accelerated computing instances.

When you look at an EC2 instance type, you will see that its name has several parts. For example, consider the T type. T is the family name, which is then followed by a number.  The number is the generation number of that type. So, a t3 instance is the third generation of the T family. In general, instance types that are of a higher generation are more powerful and provide a better value for the price. The next part of the name is the size portion of the instance. When you compare sizes, it is important to look at the coefficient portion of the size category.
For example, a t3.2xlarge has twice the vCPU and memory of a t3.xlarge. It is also important to note that network bandwidth is also tied to the size of the Amazon EC2 instance. If you will run jobs that will be very network-intensive, you might be required to increase the instance specifications to meet your needs.

Instance types vary in several ways, including: CPU type, CPU or core count, storage type, storage amount, memory amount, and network performance. T3 instances are general purpose instances that provide a baseline level of CPU performance with the ability to burst above the baseline. Use cases for this type of instance include websites and web applications, development environments. C5 instances are optimized for compute-intensive workloads, and deliver cost-effective high performance at a low price per compute ratio. Use cases include scientific modeling, batch processing. R5 instances are optimized for memory-intensive applications. Use cases include high-performance databases, data mining and analysis.

It is also important to consider your network bandwidth requirements. Each instance type provides a documented network performance level. When you launch multiple new EC2 instances, Amazon EC2 attempts to place the instances so that they are spread out across the underlying hardware by default. It does this to minimize correlated failures. However, if you want to specify specific placement criteria, you can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. 

After you have choose an AMI and an instance type, you must specify the network location where the EC2 instance will be deployed. The choice of Region must be made before you start the Launch Instance Wizard. When you launch an instance in a default VPC, AWS will assign it a public IP address by default. When you launch an instance into a nondefault VPC, the subnet has an attribute that determines whether instances launched into that subnet receive a public IP address from the public IPv4 address pool. By default, AWS will not assign a public IP address to instances that are launched in a nondefault subnet. 

It is common to use EC2 instances to run an application that must make secure API calls to other AWS services. To support these use cases, AWS enables you to attach an AWS Identity and Access Management (IAM) role to an EC2 instance. You should never store AWS credentials on an EC2 instance. An instance profile is a container for an IAM role. You can attach an IAM role when you launch the instance and also attach a role to an already running EC2 instance. 

When you create your EC2 instances, you have the option of passing user data to the instance. User data can automate the completion of installations and configurations at instance launch.

When you launch an EC2 instance, you can configure storage options. For each volume that your instance will have, you can specify the size of the disks, the volume types, and whether the storage will be retained if the instance is terminated. You can also specify if encryption should be used.

Amazon Elastic Block Store (Amazon EBS) is an easy-to-use, high-performance durable block storage service that is designed to be used with Amazon EC2 for both throughput-and transaction-intensive workloads. Only instances that are backed by Amazon EBS can be stopped. 

Amazon EC2 Instance Store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance Store works well when you must temporarily store information that changes frequently, such as buffers, caches, scratch data, and other temporary content. You can also use Instance Store for data that is replicated across a fleet of instances, such as a load balanced pool of web servers.

Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic Network File System (NFS) file system for use with AWS Cloud services and on-premises resources. 

Amazon Simple Storage Service (Amazon S3) is an object storage service that offers scalability, data availability, security, and performance. 

A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags enable you to categorize AWS resources, such as EC2 instances, in different ways. Tag keys and tag values are case-sensitive.

A security group acts as a virtual firewall that controls network traffic for one or more instances. You can add rules to each security group. Rules allow traffic to or from its associated instances. When AWS decides whether to allow traffic to reach an instance, all the rules from all the security groups that are associated with the instance are evaluated. When you define a rule, you can specify the allowable source of the network communication (inbound rules) or destination (outbound rules). The source can be an IP address, an IP address range, another security group, a gateway VPC endpoint, or anywhere (which means that all sources will be allowed). Network access control lists (network ACLs) can also be used are firewalls to protect subnets in a VPC.

Amazon EC2 uses public–key cryptography to encrypt and decrypt login information. The technology uses a public key to encrypt a piece of data, and then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. 

An instance can be in one of the following states: Pending, Running, Rebooting, Shutting down, Terminated, Stopping, Stopped.

A public IP address is an IPv4 address that is reachable from the internet. Each instance that receives a public IP address is also given an external DNS hostname. If you require a persistent public IP address, you might want to associate an Elastic IP address with the instance. To associate an Elastic IP address, you must first allocate a new Elastic IP address in the Region where the instance exists. By default, all AWS accounts are limited to five (5) Elastic IP addresses per Region because public (IPv4) internet addresses are a scarce public resource. However, this is a soft limit.

Instance metadata is data about your instance. You can view it while you are connected to the instance. To access it in a browser, go to the following URL: http://169.254.169.254/latest/meta-data/. The data can also be read programmatically, such as from a terminal window that has the cURL utility. In the terminal window, run curl http://169.254.169.254/latest/meta-data/. Any user data that is specified at instance launch can also be accessed at the following URL: http://169.254.169.254/latest/user-data. 

You can monitor your instances by using Amazon CloudWatch, which collects and processes raw data from Amazon EC2 into readable, near-real-time metrics. These statistics are recorded for a period of 15 months, so you can access historical information and gain a better perspective on how your web application or service is performing. By default, Amazon EC2 provides basic monitoring, which sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance. 
<br>
<br>
-**Amazon EC2 versus a managed service like Amazon Relational Database Service**
With Amazon RDS for SQL Server, you can meet a number of requirements and improve security, database performance, scalability, and ease of management.

Improve Manageability for SQL Server with Pre-configured Parameters, Upgrade Monitoring and Metrics, Receive DB Event Notifications, Enable Automatic Software Patching, Scale in a Few Clicks, Improve Backup and Disaster Recovery.
<br>
<br>
-**Amazon EC2 Cost Optimization**

Amazon offers different pricing models to choose from when you want to run EC2 instances:

Per second billing is only available for On-Demand Instances, Reserved Instances, and Spot Instances that run Amazon Linux or Ubuntu.
On-Demand Instances are eligible for the AWS Free Tier. They have the lowest upfront cost and the most flexibility. 
Dedicated Instances are instances that run in a virtual private cloud (VPC) on hardware that’s dedicated to a single customer. 
Reserved Instance enable you to reserve computing capacity for 1-year or 3-year term with lower hourly running costs. 
Scheduled Reserved Instances enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified duration, for a 1-year term. You pay for the time that the instances are scheduled, even if you do not use them.
Spot Instances enable you to bid on unused EC2 instances, which can lower your costs. The hourly price for a Spot Instance fluctuates depending on supply and demand. Your Spot Instance runs whenever your bid exceeds the current market price.


Each Amazon EC2 pricing model provides a different set of benefits: 

On-Demand Instances offer the most flexibility, with no long-term contract and low rates. Works well for spiky workloads or if you only need to test or run an application for a short time (for example, during application development or testing). Sometimes, your workloads are unpredictable, and On-Demand Instances are a good choice for these cases.
Spot Instances provide large scale at a significantly discounted price. Spot Instances are a good choice if your applications can tolerate interruption with a 2-minute warning notification. By default, instances are terminated, but you can configure them to stop or hibernate instead. Common use cases include fault-tolerant applications such as web servers, API backends, and big data processing. 
Reserved Instances are a good choice if you have predictable or steady-state compute needs (for example, an instance that you know you want to keep running most or all of the time for months or years).
Dedicated Hosts are a good choice when you have licensing restrictions for the software you want to run on Amazon EC2, or when you have specific compliance or regulatory requirements that preclude you from using the other deployment options.

To optimize costs, you must consider four consistent, powerful drivers:

• Right-size – Choose the right balance of instance types. Notice when servers can be either sized down or turned off, and still meet your performance requirements. AWS offers approximately 60 instance types and sizes. Right-sizing is the process of reviewing deployed resources and looking for opportunities to downsize when possible. To right size: Select the cheapest instance available that still meets your performance requirements. - Review CPU, RAM, storage, and network utilization to identify instances that could be downsized. For right-sizing, use techniques such as load testing to your advantage. - Use Amazon CloudWatch metrics and set up custom metrics.
<br>
• Increase elasticity – Design your deployments to reduce the amount of server capacity that is idle by implementing deployments that are elastic, such as deployments that use automatic scaling to handle peak loads. One form of elasticity is to create, start, or use EC2 instances when they are needed, but then to turn them off when they are not in use. Elasticity is one of the central tenets of the cloud, but customers often go through a learning process to operationalize elasticity to drive cost savings. As a rule of thumb, you should target 20–30 percent of your Amazon EC2 instances to run as On-Demand Instances or Spot Instances. 
<br>
• Optimal pricing model – Recognize the available pricing options. Analyze your usage patterns so that you can run EC2 instances with the right mix of pricing options. Consider the application architecture. Does the functionality provided by your application need to run on an EC2 virtual machine? Perhaps by making use of the AWS Lambda service instead.
<br>
• Optimize storage choices – Analyze the storage requirements of your deployments. Reduce unused storage overhead when possible, and choose less expensive storage options if they can still meet your requirements for storage performance. One way you can accomplish this is by resizing EBS volumes. There are also a variety of EBS volume types. Choose the least expensive type that still meets your performance requirements. Delete these unneeded EBS snapshots. Try to identify the most appropriate destination for specific types of data.

Tagging helps provide information about what resources are being used by whom and for what purpose. You can activate cost allocation tags in the Billing and Cost Management console, and AWS can generate a cost allocation report with usage and costs grouped by your active tags. Use AWS services such as AWS Trusted Advisor. Cost-optimization efforts are typically more successful when the responsibility for cost optimization is assigned to an individual or to a team.
<br>
<br>
-**Container Services**
Containers are a method of operating system virtualization that enables you to run an application and its dependencies in resource-isolated processes. By using containers, you can easily package an application's code, configurations, and dependencies into easy-to-use building blocks that deliver environmental consistency, operational efficiency, developer productivity, and version control. Containers are smaller than virtual machines, and do not contain an entire operating system. Instead, containers share a virtualized operating system and run as resource-isolated processes, which ensure quick, reliable, and consistent deployments. Containers deliver environmental consistency because the application’s code, configurations, and dependencies are packaged into a single object. In terms of space, container images are usually an order of magnitude smaller than virtual machines.

Docker is a software platform that packages software (such as applications) into containers. By using Docker, you can quickly deploy and scale applications into any environment. Docker is best used as a solution when you want to: Standardize environments, Reduce conflicts between language stacks and versions, Use containers as a service, Run microservices using standardized code deployments, and Require portability for data processing. 

One significant difference between containers and vm's is that virtual machines run directly on a hypervisor, but containers can run on any Linux OS if they have the appropriate kernel feature support and the Docker daemon is present. This makes containers very portable. 

You could launch one or more Amazon EC2 instances, install Docker on each instance, and manage and run the Docker containers on those Amazon EC2 instances yourself. While that is an option, AWS provides a service called Amazon Elastic Container Service (Amazon ECS) that simplifies container management. Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container management service that supports Docker containers. Amazon ECS enables you to easily run applications on a managed cluster of Amazon EC2 instances. Essential Amazon ECS features include the ability to: Launch up to tens of thousands of Docker containers in seconds, Monitor container deployment, Manage the state of the cluster that runs the containers, Schedule containers by using a built-in scheduler or a third-party scheduler (for example, Apache Mesos or Blox). Amazon ECS clusters can also use Spot Instances and Reserved Instances. To prepare your application to run on Amazon ECS, you create a task definition which is a text file that describes one or more containers, up to a maximum of ten, that form your application. It can be thought of as a blueprint for your application. A task is the instantiation of a task definition within a cluster. You can specify the number of tasks that will run on your cluster. The Amazon ECS task scheduler is responsible for placing tasks within your cluster. When Amazon ECS runs the containers that make up your task, it places them on an ECS cluster. The cluster (when you choose the EC2 launch type) consists of a group of EC2 instances each of which is running an Amazon ECS container agent.

When you create an Amazon ECS cluster, you have three options: A Networking Only cluster (powered by AWS Fargate), An EC2 Linux + Networking cluster, An EC2 Windows + Networking cluster. If you choose one of the two EC2 launch type options, you will then be prompted to choose whether the cluster EC2 instances will run as On-Demand Instances or Spot Instances. In addition, you will need to specify many details about the EC2 instances that will make up your cluster. Amazon ECS keeps track of all the CPU, memory, and other resources in your cluster. 
If you choose the networking-only Fargate launch type, then the cluster that will run your containers will be managed by AWS. The Fargate option enables you to focus on designing and building your applications.

Kubernetes is open source software for container orchestration. Kubernetes can work with many containerization technologies, including Docker. Kubernetes enables you to deploy and manage containerized applications at scale. With Kubernetes, you can run any type of containerized application by using the same toolset in both on-premises data centers and the cloud. Kubernetes manages a cluster of compute instances (called nodes). It runs containers on the cluster, which are based on where compute resources are available and the resource requirements of each container. Containers are run in logical groupings called pods. A key advantage of Kubernetes is that you can use it to run your containerized applications anywhere without needing to change your operational tooling. 

You could launch one or more Amazon EC2 instances, install Docker on each instance, install Kubernetes on the cluster, and manage and run Kubernetes yourself. While that is an option, AWS provides a service called Amazon Elastic Kubernetes Service (Amazon EKS) that simplifies the management of Kubernetes clusters. Amazon Elastic Kubernetes Service (Amazon EKS) is a managed Kubernetes service that makes it easy for you to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane. Amazon offers both Amazon ECS and Amazon EKS (they are both capable of orchestrating Docker containers) to provide customers with flexible options. 


Amazon Elastic Container Registry (Amazon ECR) is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. It is integrated with Amazon ECS, so you can store, run, and manage container images for applications that run on Amazon ECS. Specify the Amazon ECR repository in your task definition, and Amazon ECS will retrieve the appropriate images for your applications. You can transfer your container images to and from Amazon ECS via HTTPS. Your images are also automatically encrypted at rest using Amazon S3 server-side encryption.

Some key takeaways from this section include: Containers can hold everything that an application needs to run. Docker is a software platform that packages software into containers. A single application can span multiple containers. Amazon Elastic Container Service (Amazon ECS) orchestrates the execution of Docker containers. Kubernetes is open source software for container orchestration. Amazon Elastic Kubernetes Service (Amazon EKS) enables you to run Kubernetes on AWS. Amazon Elastic Container Registry (Amazon ECR) enables you to store, manage, and deploy your Docker containers.
<br>
<br>
-**Introduction to AWS Lambda** 
AWS Lambda is an event-driven, serverless compute service. Lambda enables you to run code without provisioning or managing servers. You create a Lambda function, which is the AWS resource that contains the code that you upload. You then set the Lambda function to be triggered, either on a scheduled basis or in response to an event. Your code only runs when it is triggered. You pay only for the compute time you consume—you are not charged when your code is not running.

With Lambda, there are no new languages, tools, or frameworks to learn. Lambda supports multiple programming languages. Lambda completely automates the administration. It manages all the infrastructure to run your code on highly available, fault-tolerant infrastructure, which enables you to focus on building differentiated backend services. Lambda provides built-in fault tolerance. It maintains compute capacity across multiple Availability Zones in each Region. You can orchestrate multiple Lambda functions for complex or long-running tasks by building workflows with AWS Step Functions. With Step Functions and Lambda, you can build stateful, long-running processes for applications and backends. With Lambda, you pay only for the requests that are served and the compute time that is required to run your code. 

An event source is an AWS service or a developer-created application that produces events that trigger an AWS Lambda function to run. Some services publish events to Lambda by invoking the Lambda function directly. These services that invoke Lambda functions asynchronously include, but are not limited to, Amazon S3 and Amazon CloudWatch Events. Lambda can also poll resources in other services that do not publish events to Lambda. You can invoke Lambda functions directly with the Lambda console, the Lambda API, the AWS software development kit (SDK), the AWS CLI, and AWS toolkits. The direct invocation approach can be useful, such as when you are developing a mobile app. 

AWS Lambda automatically monitors Lambda functions by using Amazon CloudWatch.

A Lambda function is the custom code that you write to process events and Lambda executes the Lambda function on your behalf. When you use the AWS Management Console to create a Lambda function, you give the function a name. Then, you specify: The runtime environment the function will use. An execution role (to grant IAM permission to the function so that it can interact with other AWS services as necessary). A trigger. Add your function code (use the provided code editor or upload a file that contains your code). Specify the memory in MB to allocate to your function (128 MB to 3,008 MB). Optionally specify environment variables, description, timeout, the specific virtual private cloud (VPC) to run the function in, tags you would like to use, and other settings. All of the above settings end up in a Lambda deployment package which is a ZIP archive that contains your function code and dependencies. 

AWS Lambda limits the amount of compute and storage resources that you can use to run and store functions. There are limits on the deployment package size of a function (250 MB). A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Using layers can help avoid reaching the size limit for deployment package. Layers are also a good way to share code and data between Lambda functions. Limits are either soft or hard. Soft limits on an account can potentially be relaxed by submitting a support ticket and providing justification for the request. Hard limits cannot be increased.

Some key takeaways from this section of the module include:
<br>
• Serverless computing enables you to build and run applications and services without provisioning or managing servers.
<br>
• AWS Lambda is a serverless compute service that provides built-in fault tolerance and automatic scaling.
<br>
• An event source is an AWS service or developer-created application that triggers a Lambda function to run.
<br>
• The maximum memory allocation for a single Lambda function is 3,008 MB. 
<br>
• The maximum execution time for a Lambda function is 15 minutes.
<br>
<br>
-**Introduction to AWS Elastic Beanstalk** 
AWS Elastic Beanstalk is a platform as a service (or PaaS) that facilitates the quick deployment, scaling, and management of your web applications and services. Upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning and load balancing to automatic scaling and monitoring application health. At the same time, you retain full control over the AWS resources that power your application, and you can access the underlying resources at any time. There is no additional charge for AWS Elastic Beanstalk. You pay for the AWS resources you create to store and run your application. You only pay for what you use, as you use it. There are no minimum fees and no upfront commitments.

AWS Elastic Beanstalk enables you to deploy your code through the AWS Management Console, the AWS Command Line Interface (AWS CLI), Visual Studio, and Eclipse. AWS Elastic Beanstalk deploys your code on Apache Tomcat for Java applications; Apache HTTP Server for PHP and Python applications; NGINX or Apache HTTP Server for Node.js applications; Passenger or Puma for Ruby applications; and Microsoft Internet Information Services (IIS) for .NET applications, Java SE, Docker, and Go.

You can improve your developer productivity by focusing on writing code instead of managing and configuring servers, databases, load balancers, firewalls, and networks. Elastic Beanstalk is difficult to outgrow. With Elastic Beanstalk, your application can handle peaks in workload or traffic while minimizing your costs. You have the freedom to select the AWS resources—such as Amazon EC2 instance type—that are optimal for your application. 
<br>
<br>





[back](../blog.html)
