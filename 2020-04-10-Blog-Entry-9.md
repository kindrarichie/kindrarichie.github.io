---
layout: default
---

## Blog Post 9

This week's coursework will again be online only due to the California Coronavirus COVID-19 Response. 

I continued the AWS training and completed module 7. 

---

### AWS Module 7: Storage

Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Cloud storage is a critical component of cloud computing because it holds the information that applications use. Some broad categories of storage include:
<br>

• Instance store, or ephemeral storage, is temporary storage that is added to your Amazon EC2 instance.
<br>
• Amazon EBS is persistent, mountable storage that can be mounted as a device to an Amazon EC2 instance. Amazon EBS can be mounted to an Amazon EC2 instance only within the same Availability Zone. Only one Amazon EC2 instance at a time can mount an Amazon EBS volume.
<br>
• Amazon EFS is a shared file system that multiple Amazon EC2 instances can mount at the same time.
<br>
• Amazon S3 is persistent storage where each file becomes an object and is available through a Uniform Resource Locator (URL); it can be accessed from anywhere.
<br>
• Amazon S3 Glacier is for cold storage for data that is not accessed frequently (for example, when you need long-term data storage for archival or compliance reasons).
<br>


#### Topics

-**Amazon EBS (Elastic Block Store)** 
<br>
Amazon EBS provides persistent block storage volumes for use with Amazon EC2 instances. Persistent storage is any data storage device that retains data after power to that device is shut off. It is also sometimes called non-volatile storage. Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure. It is designed for high availability and durability. Amazon EBS volumes provide the consistent and low-latency performance. You can scale your usage up or down within minutes. 

One critical difference between some storage types is whether they offer block-level storage or object-level storage. This difference has a major effect on the throughput, latency, and cost of your storage solution. Block storage solutions are typically faster and use less bandwidth, but they can cost more than object-level storage.

Amazon EBS enables you to create individual storage volumes and attach them to an Amazon EC2 instance. Because they are directly attached to the instances, they can provide low latency between where the data is stored and where it might be used on the instance. For this reason, they can be used to run a database with an Amazon EC2 instance. Amazon EBS volumes are included as part of the backup of your instances into Amazon Machine Images (or AMIs). AMIs are stored in Amazon S3 and can be reused to create new Amazon EC2 instances later.
A backup of an Amazon EBS volume is called a snapshot. The first snapshot is called the baseline snapshot. Any other snapshot after the baseline captures only what is different from the previous snapshot. 

Matching the correct technology to your workload is a best practice for reducing storage costs. Provisioned IOPS SSD-backed Amazon EBS volumes can give you the highest performance. Only SSDs can be used as boot volumes for EC2 instances. 

You can use Amazon EBS volumes as primary storage for data that requires frequent updates. You can also use them for throughput-intensive applications that perform continuous disk scans. Amazon EBS volumes persist independently from the running life of an EC2 instance. Use cases for EBS vary by the storage type used and whether you are using General Purpose or Provisioned IOPS. 

To provide an even higher level of data durability, Amazon EBS enables you to create point-in-time snapshots of your volumes, and you can re-create a new volume from a snapshot at any time. You can also share snapshots or even copy snapshots to different AWS Regions for even greater disaster recovery (DR) protection. You can also have encrypted Amazon EBS volumes at no additional cost. 

To provide an even higher level of data durability, Amazon EBS enables you to create point-in-time snapshots of your volumes, and you can re-create a new volume from a snapshot at any time. You can also share snapshots or even copy snapshots to different AWS Regions for even greater disaster recovery (DR) protection. For example, you can encrypt and share your snapshots from Virginia in the US to Tokyo, Japan. You can also have encrypted Amazon EBS volumes at no additional cost. Amazon EBS volumes can increase capacity (dynamically) and change to different types, so you can change from hard disk drives (HDDs) to solid state drives (SSDs). 

When you begin to estimate the cost for Amazon EBS, you must consider the following:
<br>
1. Volumes – Volume storage for all Amazon EBS volume types is charged by the amount you provision in GB per month, until you release the storage.
2. IOPS – I/O is included in the price of General Purpose SSD volumes. However, for Amazon EBS magnetic volumes, I/O is charged by the number of requests that you make to your volume. With Provisioned IOPS SSD volumes, you are also charged by the amount you provision in IOPS (multiplied by the percentage of days that you provision for the month).
3. Snapshots – Amazon EBS enables you to back up snapshots of your data to Amazon S3 for durable recovery. If you opt for Amazon EBS snapshots, the added cost is per GB-month of data stored.
4. Data transfer – When you copy Amazon EBS snapshots, you are charged for the data that is transferred across Regions. After the snapshot is copied, standard Amazon EBS snapshot charges apply for storage in the destination Region.


<br>
<br>
Section takeaways:
<br>

Amazon EBS provides block-level storage volumes for use with Amazon EC2 instances. Amazon EBS volumes are off-instance storage that persists independently from the life of an instance. They are analogous to virtual disks in the cloud. Amazon EBS provides three volume types: General Purpose SSD, Provisioned IOPS SSD, and magnetic.
The three volume types differ in performance characteristics and cost, so you can choose the right storage performance and price for the needs of your applications.
Additional benefits include replication in the same Availability Zone, easy and transparent encryption, elastic volumes, and backup by using snapshots.
<br>
<br>
-**Amazon Simple Storage Service** 
<br>
Amazon S3 is object storage (if you want to change a part of a file, you must make the change and then re-upload the entire modified file) that is built to store and retrieve any amount of data from anywhere.

Amazon S3 stores data as objects within resources that are called buckets.

Amazon S3 is a managed cloud storage solution that is designed to scale seamlessly and provide 11 9s of durability. You can store virtually as many objects as you want in a bucket. Objects can be up to 5 TB in size. By default, data in Amazon S3 is stored redundantly across multiple facilities and multiple devices in each facility. Because Amazon S3 supports objects as large as several terabytes in size, you can even store database snapshots as objects. Amazon S3 also provides low-latency access to the data over the internet by Hypertext Transfer Protocol (HTTP) or Secure HTTP (HTTPS), so you can retrieve data anytime from anywhere. You can also access Amazon S3 privately through a virtual private cloud (VPC) endpoint. You get fine-grained control over who can access your data by using AWS Identity and Access Management (IAM) policies, Amazon S3 bucket policies, and even per-object access control lists.

By default, none of your data is shared publicly. You can also encrypt your data in transit and choose to enable server-side encryption on your objects. You can access Amazon S3 through the web-based AWS Management Console; programmatically through the API and SDKs; or with third-party solutions, which use the API or the SDKs.
Amazon S3 includes event notifications that enable you to set up automatic notifications when certain events occur. Those notifications can be sent to you, or they can be used to trigger other processes, such as AWS Lambda functions. With storage class analysis, you can analyze storage access patterns and transition the right data to the right storage class. The Amazon S3 Analytics feature automatically identifies the optimal lifecycle policy to transition less frequently accessed storage to Amazon S3 Standard – Infrequent Access (Amazon S3 Standard-IA). 

Amazon S3 offers a range of object-level storage classes that are designed for different use cases. These classes include:
<br>
<br>
• Amazon S3 Standard – Amazon S3 Standard is designed for high durability, availability, and performance object storage for frequently accessed data. It delivers low latency and high throughput.
<br>
• Amazon S3 Intelligent-Tiering – The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in Amazon S3 Intelligent-Tiering. It works well for long-lived data with access patterns that are unknown or unpredictable.
<br>
• Amazon S3 Standard-Infrequent Access (Amazon S3 Standard-IA) – The Amazon S3 Standard-IA storage class is used for data that is accessed less frequently, but requires rapid access when needed. Amazon S3 Standard-IA is designed to provide the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per-GB storage price and per-GB retrieval fee. 
<br>
• Amazon S3 One Zone-Infrequent Access (Amazon S3 One Zone-IA) – Amazon S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other Amazon S3 storage classes, which store data in a minimum of three Availability Zones, Amazon S3 One Zone-IA stores data in a single Availability Zone and it costs less than Amazon S3 Standard-IA. 
<br>
• Amazon S3 Glacier – Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with—or cheaper than—on-premises solutions. To keep costs low yet suitable for varying needs, Amazon S3 Glacier provides three retrieval options that range from a few minutes to hours. 
<br>
• Amazon S3 Glacier Deep Archive – Amazon S3 Glacier Deep Archive is the lowest-cost storage class for Amazon S3. It supports long-term retention and digital preservation for data that might be accessed once or twice in a year. 

Amazon S3 stores data inside buckets. Buckets are essentially the prefix for a set of files, and must be uniquely named across all of Amazon S3 globally. Buckets are logical containers for objects. You can have one or more buckets in your account. You can control access for each bucket—who can create, delete, and list objects in the bucket. You can also view access logs for the bucket and its objects, and choose the geographical region where Amazon S3 stores the bucket and its contents. To upload your data, create a bucket in an AWS Region, and then upload almost any number of objects to the bucket. Amazon S3 refers to files as objects. As soon as you have a bucket, you can store almost any number of objects inside it. An object is composed of data and any metadata that describes that file, including a URL. To store an object in Amazon S3, you upload the file that you want to store to a bucket. When you upload a file, you can set permissions on the data and any metadata.

When you create a bucket in Amazon S3, it is associated with a specific AWS Region. When you store data in the bucket, it is redundantly stored across multiple AWS facilities within your selected Region.

Amazon S3 automatically manages the storage behind your bucket while your data grows. Amazon S3 also scales to handle a high volume of requests. 

You can access Amazon S3 through the console, AWS Command Line Interface (AWS CLI), or AWS SDK. You can also access the data in your bucket directly by using REST-based endpoints.
The endpoints support HTTP or HTTPS access. To support this type of URL-based access, Amazon S3 bucket names must be globally unique and Domain Name Server (DNS)-compliant.
Also, object keys should use characters that are safe for URLs. 

Some use cases for Amazon S3:
<br>
<br>
• As a location for any application data, Amazon S3 buckets provide a shared location for storing objects that any instances of your application can access—including applications on Amazon EC2 or even traditional servers. Because the content can be fetched directly over the internet, you can offload serving that content from your application and enable clients to directly fetch the data from Amazon S3 themselves.
<br>
• For static web hosting, Amazon S3 buckets can serve the static contents of your website, including HTML, CSS, JavaScript, and other files.
<br>
• The high durability of Amazon S3 makes it a good candidate for storing backups of your data. For greater availability and disaster recovery capability, Amazon S3 can even be configured to support cross-Region replication.

Amazon S3 common scenarios:
<br>
<br>
Backup and storage – Provide data backup and storage services for others 
<br>
Application hosting – Provide services that deploy, install, and manage web applications
<br>
Media hosting – Build a redundant, scalable, and highly available infrastructure that hosts video, photo, or music uploads and downloads
<br>
Software delivery – Host your software applications that customers can download

With Amazon S3, specific costs vary depending on the Region and the specific requests that were made. As a general rule, you pay only for transfers that cross the boundary of your Region, which means you do not pay for transfers in to Amazon S3 or transfers out from Amazon S3 to Amazon CloudFront edge locations within that same Region.

When you begin to estimate the costs of Amazon S3, you must consider the following:
<br>
1. Storage class type – Standard storage is designed to provide 11 9s of durability and four 9s of availability. S3 Standard – Infrequent Access (S-IA) is a storage option within Amazon S3 that you can use to reduce your costs by storing less frequently accessed data at slightly lower levels of redundancy than Amazon S3 standard storage. 
2. Amount of storage – The number and size of objects stored in your Amazon S3 buckets.
3. Requests – Consider the number and type of requests. GET requests incur charges at different rates than other requests, such as PUT and COPY requests. GET – Retrieves an object from Amazon S3. You must have READ access to use this operation. PUT – Adds an object to a bucket. You must have WRITE permissions on a bucket to add an object to it. COPY – Creates a copy of an object that is already stored in Amazon S3. A COPY operation is the same as performing a GET and then a PUT.
4. Data transfer – Consider the amount of data that is transferred out of the Amazon S3 Region. Remember that data transfer in is free, but there is a charge for data transfer out.
<br>
<br>

-**Amazon Elastic File System (Amazon EFS)** 
<br>
Amazon EFS implements storage for EC2 instances that multiple virtual machines can access at the same time. It is implemented as a shared file system that uses the Network File System (NFS) protocol. Amazon Elastic File System (Amazon EFS) provides simple, scalable, elastic file storage for use with AWS services and on-premises resources. Amazon EFS is built to dynamically scale on demand without disrupting applications.

Thousands of Amazon EC2 instances can access an Amazon EFS file system at the same time. Amazon EFS is also designed to be highly durable and highly available. Amazon EFS requires no minimum fee or setup costs, and you pay only for the storage that you use.

With Amazon EFS, you can create a file system, mount the file system on an Amazon EC2 instance, and then read and write data from to and from your file system. You can access your Amazon EFS file system concurrently from Amazon EC2 instances in your VPC, so applications that scale beyond a single connection can access a file system. Amazon EC2 instances that run in multiple Availability Zones within the same AWS Region can access the file system. We recommend that you access the file system from a mount target within the same Availability Zone.

Complete five steps to create and use your first Amazon EFS file system, mount it on an Amazon EC2 instance in your VPC, and test the end-to-end setup:
<br>
<br>
-Create your Amazon EC2 resources and launch your instance. (Before you can launch and connect to an Amazon EC2 instance, you must create a key pair, unless you already have one.)
<br>
-Create your Amazon EFS file system. 
<br>
-In the appropriate subnets, create your mount targets. 
<br>
-Next, connect to your Amazon EC2 instance and mount the Amazon EFS file system. 
<br>
-Finally, clean up your resources and protect your AWS account.
<br>

In Amazon EFS, a file system is the primary resource. Each file system has properties such as: ID,  Creation token, Creation time, File system size in bytes, Number of mount targets that are created for the file system, and File system state. 

Amazon EFS also supports other resources to configure the primary resource. These resources include mount targets and tags.

Mount target: To access your file system, you must create mount targets in your VPC. Each mount target has the following properties: The mount target ID, The subnet ID for the subnet where it was created, The file system ID for the file system where it was created, An IP address where the file system can be mounted, and The mount target state.
<br>

You can use the IP address or the Domain Name System (DNS) name in your mount command.
<br>
<br>
-**Amazon S3 Glacier** 
<br>
Amazon S3 Glacier is a secure, durable, and extremely low-cost cloud storage service for data archiving and long-term backup. You can store your data at an extremely low cost (even in comparison to Amazon S3), but you cannot retrieve your data immediately when you want it.

Three key Amazon S3 Glacier terms you should be familiar with: 
<br>
• Archive – Any object (such as a photo, video, file, or document) that you store in Amazon S3 Glacier. It is the base unit of storage in Amazon S3 Glacier. Each archive has its own unique ID and it can also have a description.
<br>
• Vault – A container for storing archives. When you create a vault, you specify the vault name and the Region where you want to locate the vault.
<br>
• Vault access policy – Determine who can and cannot access the data that is stored in the vault, and what operations users can and cannot perform. One vault access permissions policy can be created for each vault to manage access permissions for that vault. You can also use a vault lock policy to make sure that a vault cannot be altered. Each vault can have one vault access policy and one vault lock policy that are attached to it.

Three options for retrieving data:
<br>
 • Expedited retrievals are typically made available within 1–5 minutes (highest cost). 
 <br>
 • Standard retrievals typically complete within 3–5 hours (less time than expedited, more time than bulk).
 <br>
• Bulk retrievals typically complete within 5–12 hours (lowest cost).

Use Cases:
<br>
Media asset archiving
<br>
Healthcare information archiving
<br>
Regulatory and compliance archiving 
<br>
Scientific data archiving
<br>
Digital preservation
<br>
Magnetic tape replacement 

To store and access data in Amazon S3 Glacier, you can use the AWS Management Console. For almost all other operations and interactions with Amazon S3 Glacier, you must use either the Amazon S3 Glacier REST APIs, the AWS Java or .NET SDKs, or the AWS CLI.

You should automate the lifecycle of the data that you store in Amazon S3. By using lifecycle policies, you can cycle data at regular intervals between different Amazon S3 storage types. This automation reduces your overall cost, because you pay less for data as it becomes less important with time. In addition to setting lifecycle rules per object, you can also set lifecycle rules per bucket.

Amazon S3 and Amazon S3 Glacier are both object storage solutions that enable you to store a virtually unlimited amount of data. Some critical differences between them: 
<br>
1. Be careful when you decide which storage solution is correct for your needs. These two services serve very different storage needs. Amazon S3 is designed for frequent, low-latency access to your data, but Amazon S3 Glacier is designed for low-cost, long-term storage of infrequently accessed data.
2. The maximum item size in Amazon S3 is 5 TB, but Amazon S3 Glacier can store items that are up to 40 TB.
3. Because Amazon S3 gives you faster access to your data, the storage cost per gigabyte is higher than it is with Amazon S3 Glacier.
4. While both services have per-request charges, Amazon S3 charges for PUT, COPY, POST, LIST, GET operations. In contrast, Amazon S3 Glacier charges for UPLOAD and retrieval operations.
5. Because Amazon S3 Glacier was designed for less-frequent access to data, it costs more for each retrieval request than Amazon S3.
6. With both solutions, you can securely transfer your data over HTTPS. Any data that is archived in Amazon S3 Glacier is encrypted by default. With Amazon S3, your application must initiate server-side encryption. You can accomplish server-side encryption in Amazon S3 in several ways:

    -Server-side encryption with Amazon S3-managed encryption keys (SSE-S3) employs strong multi-factor encryption.
    <br>
    -Using server-side encryption with Customer-provided Encryption Keys (SSE-C) enables you to set your own encryption keys.
    <br>
    -Using server-side encryption with AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system that is scaled for the cloud. AWS KMS uses Customer Master Keys (CMKs) to encrypt your Amazon S3 objects. You use AWS KMS through the Encryption Keys section in the IAM console. 
    
By default, only you can access your data. You can enable and control access to your data in Amazon S3 Glacier by using IAM. You set up an IAM policy that specifies user access.

Key Takeaways:
<br>
-S3 Glacier data is a data archiving service designed for security, durability, and a low cost.
<br>
-S3 Glacier pricing is based on region.
<br>
-Long term storage
<br>
-11 9s of durability for objects. 
<br>
<br>




















<br>
<br>





[back](../blog.html)
