---
layout: default
---

## Blog Post 9

This week's coursework will again be online only due to the California Coronavirus COVID-19 Response. 

I continued the AWS training and completed module 7. 

---

### AWS Module 7: Storage

Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems. Cloud storage is a critical component of cloud computing because it holds the information that applications use. Some broad categories of storage include:
<br>

• Instance store, or ephemeral storage, is temporary storage that is added to your Amazon EC2 instance.
<br>
• Amazon EBS is persistent, mountable storage that can be mounted as a device to an Amazon EC2 instance. Amazon EBS can be mounted to an Amazon EC2 instance only within the same Availability Zone. Only one Amazon EC2 instance at a time can mount an Amazon EBS volume.
<br>
• Amazon EFS is a shared file system that multiple Amazon EC2 instances can mount at the same time.
<br>
• Amazon S3 is persistent storage where each file becomes an object and is available through a Uniform Resource Locator (URL); it can be accessed from anywhere.
<br>
• Amazon S3 Glacier is for cold storage for data that is not accessed frequently (for example, when you need long-term data storage for archival or compliance reasons).
<br>


#### Topics

-**Amazon EBS (Elastic Block Store)** 
<br>
Amazon EBS provides persistent block storage volumes for use with Amazon EC2 instances. Persistent storage is any data storage device that retains data after power to that device is shut off. It is also sometimes called non-volatile storage. Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure. It is designed for high availability and durability. Amazon EBS volumes provide the consistent and low-latency performance. You can scale your usage up or down within minutes. 

One critical difference between some storage types is whether they offer block-level storage or object-level storage. This difference has a major effect on the throughput, latency, and cost of your storage solution. Block storage solutions are typically faster and use less bandwidth, but they can cost more than object-level storage.

Amazon EBS enables you to create individual storage volumes and attach them to an Amazon EC2 instance. Because they are directly attached to the instances, they can provide low latency between where the data is stored and where it might be used on the instance. For this reason, they can be used to run a database with an Amazon EC2 instance. Amazon EBS volumes are included as part of the backup of your instances into Amazon Machine Images (or AMIs). AMIs are stored in Amazon S3 and can be reused to create new Amazon EC2 instances later.
A backup of an Amazon EBS volume is called a snapshot. The first snapshot is called the baseline snapshot. Any other snapshot after the baseline captures only what is different from the previous snapshot. 

Matching the correct technology to your workload is a best practice for reducing storage costs. Provisioned IOPS SSD-backed Amazon EBS volumes can give you the highest performance. Only SSDs can be used as boot volumes for EC2 instances. 

You can use Amazon EBS volumes as primary storage for data that requires frequent updates. You can also use them for throughput-intensive applications that perform continuous disk scans. Amazon EBS volumes persist independently from the running life of an EC2 instance. Use cases for EBS vary by the storage type used and whether you are using General Purpose or Provisioned IOPS. 

To provide an even higher level of data durability, Amazon EBS enables you to create point-in-time snapshots of your volumes, and you can re-create a new volume from a snapshot at any time. You can also share snapshots or even copy snapshots to different AWS Regions for even greater disaster recovery (DR) protection. You can also have encrypted Amazon EBS volumes at no additional cost. 

To provide an even higher level of data durability, Amazon EBS enables you to create point-in-time snapshots of your volumes, and you can re-create a new volume from a snapshot at any time. You can also share snapshots or even copy snapshots to different AWS Regions for even greater disaster recovery (DR) protection. For example, you can encrypt and share your snapshots from Virginia in the US to Tokyo, Japan. You can also have encrypted Amazon EBS volumes at no additional cost. Amazon EBS volumes can increase capacity (dynamically) and change to different types, so you can change from hard disk drives (HDDs) to solid state drives (SSDs). 

When you begin to estimate the cost for Amazon EBS, you must consider the following:
<br>
1. Volumes – Volume storage for all Amazon EBS volume types is charged by the amount you provision in GB per month, until you release the storage.
2. IOPS – I/O is included in the price of General Purpose SSD volumes. However, for Amazon EBS magnetic volumes, I/O is charged by the number of requests that you make to your volume. With Provisioned IOPS SSD volumes, you are also charged by the amount you provision in IOPS (multiplied by the percentage of days that you provision for the month).
3. Snapshots – Amazon EBS enables you to back up snapshots of your data to Amazon S3 for durable recovery. If you opt for Amazon EBS snapshots, the added cost is per GB-month of data stored.
4. Data transfer – When you copy Amazon EBS snapshots, you are charged for the data that is transferred across Regions. After the snapshot is copied, standard Amazon EBS snapshot charges apply for storage in the destination Region.


<br>
Section takeaways:
<br>

Amazon EBS provides block-level storage volumes for use with Amazon EC2 instances. Amazon EBS volumes are off-instance storage that persists independently from the life of an instance. They are analogous to virtual disks in the cloud. Amazon EBS provides three volume types: General Purpose SSD, Provisioned IOPS SSD, and magnetic.
The three volume types differ in performance characteristics and cost, so you can choose the right storage performance and price for the needs of your applications.
Additional benefits include replication in the same Availability Zone, easy and transparent encryption, elastic volumes, and backup by using snapshots.
<br>
<br>
-**Amazon Simple Storage Service** 
<br>
Amazon S3 is object storage (if you want to change a part of a file, you must make the change and then re-upload the entire modified file) that is built to store and retrieve any amount of data from anywhere.

Amazon S3 stores data as objects within resources that are called buckets.

Amazon S3 is a managed cloud storage solution that is designed to scale seamlessly and provide 11 9s of durability. You can store virtually as many objects as you want in a bucket. Objects can be up to 5 TB in size. By default, data in Amazon S3 is stored redundantly across multiple facilities and multiple devices in each facility. Because Amazon S3 supports objects as large as several terabytes in size, you can even store database snapshots as objects. Amazon S3 also provides low-latency access to the data over the internet by Hypertext Transfer Protocol (HTTP) or Secure HTTP (HTTPS), so you can retrieve data anytime from anywhere. You can also access Amazon S3 privately through a virtual private cloud (VPC) endpoint. You get fine-grained control over who can access your data by using AWS Identity and Access Management (IAM) policies, Amazon S3 bucket policies, and even per-object access control lists.

By default, none of your data is shared publicly. You can also encrypt your data in transit and choose to enable server-side encryption on your objects. You can access Amazon S3 through the web-based AWS Management Console; programmatically through the API and SDKs; or with third-party solutions, which use the API or the SDKs.
Amazon S3 includes event notifications that enable you to set up automatic notifications when certain events occur. Those notifications can be sent to you, or they can be used to trigger other processes, such as AWS Lambda functions. With storage class analysis, you can analyze storage access patterns and transition the right data to the right storage class. The Amazon S3 Analytics feature automatically identifies the optimal lifecycle policy to transition less frequently accessed storage to Amazon S3 Standard – Infrequent Access (Amazon S3 Standard-IA). 

Amazon S3 offers a range of object-level storage classes that are designed for different use cases. These classes include:
<br>
<br>
• Amazon S3 Standard – Amazon S3 Standard is designed for high durability, availability, and performance object storage for frequently accessed data. It delivers low latency and high throughput.
<br>
• Amazon S3 Intelligent-Tiering – The Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in Amazon S3 Intelligent-Tiering. It works well for long-lived data with access patterns that are unknown or unpredictable.
<br>
• Amazon S3 Standard-Infrequent Access (Amazon S3 Standard-IA) – The Amazon S3 Standard-IA storage class is used for data that is accessed less frequently, but requires rapid access when needed. Amazon S3 Standard-IA is designed to provide the high durability, high throughput, and low latency of Amazon S3 Standard, with a low per-GB storage price and per-GB retrieval fee. 
<br>
• Amazon S3 One Zone-Infrequent Access (Amazon S3 One Zone-IA) – Amazon S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other Amazon S3 storage classes, which store data in a minimum of three Availability Zones, Amazon S3 One Zone-IA stores data in a single Availability Zone and it costs less than Amazon S3 Standard-IA. 
<br>
• Amazon S3 Glacier – Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with—or cheaper than—on-premises solutions. To keep costs low yet suitable for varying needs, Amazon S3 Glacier provides three retrieval options that range from a few minutes to hours. 
<br>
• Amazon S3 Glacier Deep Archive – Amazon S3 Glacier Deep Archive is the lowest-cost storage class for Amazon S3. It supports long-term retention and digital preservation for data that might be accessed once or twice in a year. 

Amazon S3 stores data inside buckets. Buckets are essentially the prefix for a set of files, and must be uniquely named across all of Amazon S3 globally. Buckets are logical containers for objects. You can have one or more buckets in your account. You can control access for each bucket—who can create, delete, and list objects in the bucket. You can also view access logs for the bucket and its objects, and choose the geographical region where Amazon S3 stores the bucket and its contents. To upload your data, create a bucket in an AWS Region, and then upload almost any number of objects to the bucket. Amazon S3 refers to files as objects. As soon as you have a bucket, you can store almost any number of objects inside it. An object is composed of data and any metadata that describes that file, including a URL. To store an object in Amazon S3, you upload the file that you want to store to a bucket. When you upload a file, you can set permissions on the data and any metadata.

When you create a bucket in Amazon S3, it is associated with a specific AWS Region. When you store data in the bucket, it is redundantly stored across multiple AWS facilities within your selected Region.

Amazon S3 automatically manages the storage behind your bucket while your data grows. Amazon S3 also scales to handle a high volume of requests. 

You can access Amazon S3 through the console, AWS Command Line Interface (AWS CLI), or AWS SDK. You can also access the data in your bucket directly by using REST-based endpoints.
The endpoints support HTTP or HTTPS access. To support this type of URL-based access, Amazon S3 bucket names must be globally unique and Domain Name Server (DNS)-compliant.
Also, object keys should use characters that are safe for URLs. 

Some use cases for Amazon S3:
<br>
<br>
• As a location for any application data, Amazon S3 buckets provide a shared location for storing objects that any instances of your application can access—including applications on Amazon EC2 or even traditional servers. Because the content can be fetched directly over the internet, you can offload serving that content from your application and enable clients to directly fetch the data from Amazon S3 themselves.
<br>
• For static web hosting, Amazon S3 buckets can serve the static contents of your website, including HTML, CSS, JavaScript, and other files.
<br>
• The high durability of Amazon S3 makes it a good candidate for storing backups of your data. For greater availability and disaster recovery capability, Amazon S3 can even be configured to support cross-Region replication.

Amazon S3 common scenarios:
<br>
<br>
Backup and storage – Provide data backup and storage services for others 
<br>
Application hosting – Provide services that deploy, install, and manage web applications
<br>
Media hosting – Build a redundant, scalable, and highly available infrastructure that hosts video, photo, or music uploads and downloads
<br>
Software delivery – Host your software applications that customers can download

With Amazon S3, specific costs vary depending on the Region and the specific requests that were made. As a general rule, you pay only for transfers that cross the boundary of your Region, which means you do not pay for transfers in to Amazon S3 or transfers out from Amazon S3 to Amazon CloudFront edge locations within that same Region.

When you begin to estimate the costs of Amazon S3, you must consider the following:
<br>
1. Storage class type – Standard storage is designed to provide 11 9s of durability and four 9s of availability. S3 Standard – Infrequent Access (S-IA) is a storage option within Amazon S3 that you can use to reduce your costs by storing less frequently accessed data at slightly lower levels of redundancy than Amazon S3 standard storage. 
2. Amount of storage – The number and size of objects stored in your Amazon S3 buckets.
3. Requests – Consider the number and type of requests. GET requests incur charges at different rates than other requests, such as PUT and COPY requests. GET – Retrieves an object from Amazon S3. You must have READ access to use this operation. PUT – Adds an object to a bucket. You must have WRITE permissions on a bucket to add an object to it. COPY – Creates a copy of an object that is already stored in Amazon S3. A COPY operation is the same as performing a GET and then a PUT.
4. Data transfer – Consider the amount of data that is transferred out of the Amazon S3 Region. Remember that data transfer in is free, but there is a charge for data transfer out.
<br>
<br>

-**Amazon Elastic File System (Amazon EFS)** 
<br>
Amazon EFS implements storage for EC2 instances that multiple virtual machines can access at the same time. It is implemented as a shared file system that uses the Network File System (NFS) protocol.



<br>
<br>





[back](../blog.html)
